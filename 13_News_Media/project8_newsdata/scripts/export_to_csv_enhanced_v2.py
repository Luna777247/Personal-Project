import json
import pandas as pd
import re
import glob
from datetime import datetime
import spacy
from transformers import pipeline
import unicodedata
import logging
from tqdm import tqdm
import multiprocessing as mp
from functools import lru_cache
import numpy as np
from collections import Counter
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# ==================== CONFIGURATION ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('disaster_processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==================== MODEL INITIALIZATION ====================
class ModelManager:
    """Qu·∫£n l√Ω c√°c models NLP v·ªõi lazy loading"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.nlp_spacy = None
            self.ner_pipeline = None
            self.initialized = True
            self._load_models()
    
    def _load_models(self):
        """Load models v·ªõi error handling"""
        # Load spaCy
        try:
            self.nlp_spacy = spacy.load("vi_core_news_lg")
            logger.info("‚úÖ Loaded Vietnamese spaCy model")
        except:
            try:
                self.nlp_spacy = spacy.load("en_core_web_sm")
                logger.warning("‚ö†Ô∏è Using English spaCy fallback")
            except:
                logger.warning("‚ùå No spaCy model available")
        
        # Load Hugging Face NER
        try:
            self.ner_pipeline = pipeline(
                "ner",
                model="NlpHUST/ner-vietnamese-electra-base",
                aggregation_strategy="simple",
                device=-1  # CPU
            )
            logger.info("‚úÖ Loaded Hugging Face NER model")
        except:
            logger.warning("‚ùå Hugging Face NER unavailable")

models = ModelManager()

# ==================== ENHANCED REGEX PATTERNS ====================
class PatternLibrary:
    """Th∆∞ vi·ªán patterns ƒë∆∞·ª£c t·ªëi ∆∞u v√† m·ªü r·ªông"""
    
    # Weather patterns
    WEATHER = {
        'wind_speed': [
            re.compile(r'(?:s·ª©c\s+gi√≥|t·ªëc\s+ƒë·ªô\s+gi√≥|gi√≥\s+m·∫°nh)\s+(?:c·∫•p\s+)?(\d+)', re.IGNORECASE),
            re.compile(r'gi√≥\s+(?:c·∫•p|cap)\s*(\d+)', re.IGNORECASE),
            re.compile(r'(?:m·∫°nh|c·ª±c\s+ƒë·∫°i)\s+c·∫•p\s*(\d+)', re.IGNORECASE),
        ],
        'rainfall': [
            re.compile(r'(?:l∆∞·ª£ng\s+m∆∞a|m∆∞a)\s+(?:ph·ªï\s+bi·∫øn\s+)?(\d+)(?:-(\d+))?\s*mm', re.IGNORECASE),
            re.compile(r't·ªïng\s+l∆∞·ª£ng\s+m∆∞a\s+(\d+)(?:-(\d+))?\s*mm', re.IGNORECASE),
            re.compile(r'm∆∞a\s+t·ª´\s+(\d+)\s+ƒë·∫øn\s+(\d+)\s*mm', re.IGNORECASE),
        ],
        'temperature': [
            re.compile(r'nhi·ªát\s+ƒë·ªô\s+(?:cao\s+nh·∫•t\s+)?(\d+)(?:-(\d+))?\s*¬∞?[Cc]', re.IGNORECASE),
            re.compile(r'n√≥ng\s+(?:l√™n\s+)?(?:ƒë·∫øn\s+)?(\d+)\s*ƒë·ªô', re.IGNORECASE),
        ]
    }
    
    # Damage patterns
    DAMAGE = {
        'casualties': [
            re.compile(r'(\d+(?:[.,]\d+)?)\s*(?:ng∆∞·ªùi|ca)\s*(?:ch·∫øt|t·ª≠\s+vong|thi·ªát\s+m·∫°ng)', re.IGNORECASE),
            re.compile(r'(?:l√†m\s+)?ch·∫øt\s+(\d+)\s*ng∆∞·ªùi', re.IGNORECASE),
            re.compile(r's·ªë\s+ng∆∞·ªùi\s+ch·∫øt\s*[:;]?\s*(\d+)', re.IGNORECASE),
        ],
        'injured': [
            re.compile(r'(\d+)\s*(?:ng∆∞·ªùi|ca)\s*(?:b·ªã\s+th∆∞∆°ng|th∆∞∆°ng\s+vong)', re.IGNORECASE),
        ],
        'missing': [
            re.compile(r'(\d+)\s*ng∆∞·ªùi\s*(?:m·∫•t\s+t√≠ch|b·ªã\s+m·∫•t\s+t√≠ch)', re.IGNORECASE),
        ],
        'economic': [
            re.compile(r'(?:thi·ªát\s+h·∫°i|t·ªïn\s+th·∫•t)\s*(?:kho·∫£ng\s+)?(\d+(?:[.,]\d+)?)\s*(t·ª∑|tri·ªáu|ngh√¨n)?\s*(?:ƒë·ªìng|USD|\$)?', re.IGNORECASE),
            re.compile(r'(\d+(?:[.,]\d+)?)\s*(t·ª∑|tri·ªáu)\s*ƒë·ªìng\s*thi·ªát\s+h·∫°i', re.IGNORECASE),
        ],
        'houses': [
            re.compile(r'(\d+(?:[.,]\d+)?)\s*(?:cƒÉn|ng√¥i)\s*nh√†\s*(?:b·ªã\s+)?(?:s·∫≠p|h∆∞\s+h·ªèng|t·ªëc\s+m√°i)', re.IGNORECASE),
            re.compile(r'h∆∞\s+h·ªèng\s+(\d+(?:[.,]\d+)?)\s*(?:cƒÉn|ng√¥i)\s*nh√†', re.IGNORECASE),
        ]
    }
    
    # Location patterns (Vietnamese specific)
    LOCATIONS = {
        'provinces': [
            'H√† N·ªôi', 'TP.HCM', 'H·ªì Ch√≠ Minh', 'ƒê√† N·∫µng', 'H·∫£i Ph√≤ng', 'C·∫ßn Th∆°',
            'Qu·∫£ng Ninh', 'Thanh H√≥a', 'Ngh·ªá An', 'H√† Tƒ©nh', 'Qu·∫£ng B√¨nh', 'Qu·∫£ng Tr·ªã',
            'Th·ª´a Thi√™n Hu·∫ø', 'Qu·∫£ng Nam', 'Qu·∫£ng Ng√£i', 'B√¨nh ƒê·ªãnh', 'Ph√∫ Y√™n',
            'Kh√°nh H√≤a', 'Ninh Thu·∫≠n', 'B√¨nh Thu·∫≠n', 'Kon Tum', 'Gia Lai', 'ƒê·∫Øk L·∫Øk',
            'ƒê·∫Øk N√¥ng', 'L√¢m ƒê·ªìng', 'B√¨nh Ph∆∞·ªõc', 'T√¢y Ninh', 'B√¨nh D∆∞∆°ng', 'ƒê·ªìng Nai',
            'B√† R·ªãa V≈©ng T√†u', 'Long An', 'Ti·ªÅn Giang', 'B·∫øn Tre', 'Tr√† Vinh',
            'Vƒ©nh Long', 'ƒê·ªìng Th√°p', 'An Giang', 'Ki√™n Giang', 'C√† Mau', 'B·∫°c Li√™u',
            'S√≥c TrƒÉng', 'H·∫≠u Giang', 'L√†o Cai', 'Y√™n B√°i', 'S∆°n La', 'Lai Ch√¢u',
            'ƒêi·ªán Bi√™n', 'H√≤a B√¨nh', 'Th√°i Nguy√™n', 'B·∫Øc K·∫°n', 'Cao B·∫±ng', 'L·∫°ng S∆°n',
            'H√† Giang', 'Tuy√™n Quang', 'Ph√∫ Th·ªç', 'Vƒ©nh Ph√∫c', 'B·∫Øc Ninh', 'B·∫Øc Giang',
            'H·∫£i D∆∞∆°ng', 'H∆∞ng Y√™n', 'Th√°i B√¨nh', 'H√† Nam', 'Nam ƒê·ªãnh', 'Ninh B√¨nh'
        ],
        'regions': [
            'Mi·ªÅn B·∫Øc', 'Mi·ªÅn Trung', 'Mi·ªÅn Nam', 'B·∫Øc B·ªô', 'Trung B·ªô', 'Nam B·ªô',
            'ƒê·ªìng b·∫±ng s√¥ng H·ªìng', 'ƒê·ªìng b·∫±ng s√¥ng C·ª≠u Long', 'T√¢y Nguy√™n',
            'Trung du mi·ªÅn n√∫i B·∫Øc B·ªô', 'Duy√™n h·∫£i Nam Trung B·ªô', 'ƒê√¥ng Nam B·ªô'
        ],
        'seas': ['Bi·ªÉn ƒê√¥ng', 'Ho√†ng Sa', 'Tr∆∞·ªùng Sa', 'V·ªãnh B·∫Øc B·ªô', 'V·ªãnh H·∫° Long']
    }

patterns = PatternLibrary()

# ==================== EXTRACTION FUNCTIONS ====================
class DataExtractor:
    """Class ch·ª©a c√°c h√†m tr√≠ch xu·∫•t ƒë∆∞·ª£c t·ªëi ∆∞u"""
    
    @staticmethod
    @lru_cache(maxsize=1000)
    def clean_text(text: str) -> str:
        """L√†m s·∫°ch text v·ªõi caching"""
        text = unicodedata.normalize('NFKC', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    @staticmethod
    def extract_location_nlp(content: str) -> List[str]:
        """Tr√≠ch xu·∫•t location b·∫±ng NLP models"""
        locations = []
        
        # spaCy extraction
        if models.nlp_spacy:
            try:
                doc = models.nlp_spacy(content[:5000])  # Limit length
                for ent in doc.ents:
                    if ent.label_ in ['GPE', 'LOC', 'FAC']:
                        locations.append(ent.text)
            except Exception as e:
                logger.debug(f"spaCy extraction failed: {e}")
        
        # Hugging Face NER
        if models.ner_pipeline:
            try:
                clean_content = DataExtractor.clean_text(content[:1000])
                entities = models.ner_pipeline(clean_content)
                for ent in entities:
                    if ent['entity_group'] == 'LOC' and not ent['word'].startswith('##'):
                        locations.append(ent['word'])
            except Exception as e:
                logger.debug(f"HF NER failed: {e}")
        
        return list(set(locations))
    
    @staticmethod
    def extract_location_rule(content: str) -> List[str]:
        """Rule-based location extraction"""
        found = []
        content_lower = content.lower()
        
        # Check provinces
        for province in patterns.LOCATIONS['provinces']:
            if province.lower() in content_lower:
                found.append(province)
        
        # Check regions
        for region in patterns.LOCATIONS['regions']:
            if region.lower() in content_lower:
                found.append(region)
        
        # Check seas
        for sea in patterns.LOCATIONS['seas']:
            if sea.lower() in content_lower:
                found.append(sea)
        
        return list(set(found))
    
    @staticmethod
    def extract_location(content: str) -> Optional[str]:
        """K·∫øt h·ª£p NLP v√† rule-based ƒë·ªÉ tr√≠ch xu·∫•t location"""
        nlp_locs = DataExtractor.extract_location_nlp(content)
        rule_locs = DataExtractor.extract_location_rule(content)
        
        # Merge v√† ∆∞u ti√™n rule-based (reliable h∆°n cho VN locations)
        all_locs = list(set(rule_locs + nlp_locs))
        
        # Filter: ch·ªâ gi·ªØ locations h·ª£p l·ªá
        valid_locs = []
        for loc in all_locs:
            loc_clean = loc.strip()
            if (len(loc_clean) >= 3 and 
                not loc_clean.startswith('##') and 
                not re.match(r'^\W+$', loc_clean)):
                valid_locs.append(loc_clean)
        
        return ', '.join(valid_locs[:5]) if valid_locs else None  # Limit to 5 locations
    
    @staticmethod
    def extract_with_patterns(content: str, pattern_list: List) -> Optional[str]:
        """Extract data using multiple patterns"""
        for pattern in pattern_list:
            match = pattern.search(content)
            if match:
                return match
        return None
    
    @staticmethod
    def extract_wind_speed(content: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t s·ª©c gi√≥"""
        match = DataExtractor.extract_with_patterns(content, patterns.WEATHER['wind_speed'])
        if match:
            level = match.group(1)
            # Check for gust
            gust_match = re.search(r'gi·∫≠t\s+c·∫•p\s*(\d+)', content, re.IGNORECASE)
            if gust_match:
                return f"c·∫•p {level}, gi·∫≠t c·∫•p {gust_match.group(1)}"
            return f"c·∫•p {level}"
        return None
    
    @staticmethod
    def extract_rainfall(content: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t l∆∞·ª£ng m∆∞a"""
        match = DataExtractor.extract_with_patterns(content, patterns.WEATHER['rainfall'])
        if match:
            if match.group(2):  # Range
                return f"{match.group(1)}-{match.group(2)} mm"
            return f"{match.group(1)} mm"
        return None
    
    @staticmethod
    def extract_casualties(content: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t thi·ªát h·∫°i v·ªÅ ng∆∞·ªùi"""
        parts = []
        
        # Deaths
        match = DataExtractor.extract_with_patterns(content, patterns.DAMAGE['casualties'])
        if match:
            num = match.group(1).replace(',', '.')
            parts.append(f"{num} ng∆∞·ªùi ch·∫øt")
        
        # Injured
        match = DataExtractor.extract_with_patterns(content, patterns.DAMAGE['injured'])
        if match:
            parts.append(f"{match.group(1)} ng∆∞·ªùi b·ªã th∆∞∆°ng")
        
        # Missing
        match = DataExtractor.extract_with_patterns(content, patterns.DAMAGE['missing'])
        if match:
            parts.append(f"{match.group(1)} ng∆∞·ªùi m·∫•t t√≠ch")
        
        return ', '.join(parts) if parts else None
    
    @staticmethod
    def extract_damages(content: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t thi·ªát h·∫°i kinh t·∫ø"""
        match = DataExtractor.extract_with_patterns(content, patterns.DAMAGE['economic'])
        if match:
            amount = match.group(1).replace(',', '.')
            unit = match.group(2) if len(match.groups()) >= 2 and match.group(2) else 't·ª∑'
            return f"{amount} {unit} ƒë·ªìng"
        return None
    
    @staticmethod
    def extract_event_name(content: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t t√™n s·ª± ki·ªán v·ªõi validation ch·∫∑t ch·∫Ω"""
        # Storm names
        storm_patterns = [
            r'(?:b√£o|c∆°n\s+b√£o)\s+([A-Z][a-z]{2,12})',
            r'mang\s+t√™n\s+([A-Z][a-z]{2,12})',
        ]
        
        exclude_words = {'Nhi', 'G·ªìm', 'Gi√¥ng', 'V√πng', 'Khu', 'T·∫°i', 'Nhi·ªÅu', 'L·ªõn'}
        
        for pattern in storm_patterns:
            match = re.search(pattern, content)
            if match:
                name = match.group(1).strip()
                if (len(name) >= 3 and 
                    name not in exclude_words and
                    not any(char.isdigit() for char in name)):
                    return f"B√£o {name}"
        
        # Other disasters
        disaster_types = [
            ('ƒë·ªông ƒë·∫•t', 'ƒê·ªông ƒë·∫•t'),
            ('s√≥ng th·∫ßn', 'S√≥ng th·∫ßn'),
            ('n√∫i l·ª≠a', 'N√∫i l·ª≠a'),
            ('ch√°y r·ª´ng', 'Ch√°y r·ª´ng'),
            ('l≈© l·ª•t', 'L≈© l·ª•t'),
        ]
        
        for keyword, prefix in disaster_types:
            pattern = re.compile(rf'{keyword}\s+([^,\.\d]{{3,20}})', re.IGNORECASE)
            match = pattern.search(content)
            if match:
                name = match.group(1).strip()
                if len(name) >= 3 and len(name) <= 20:
                    return f"{prefix} {name}"
        
        return None
    
    @staticmethod
    def calculate_severity(wind_speed: Optional[str], disaster_type: str) -> str:
        """T√≠nh m·ª©c ƒë·ªô nghi√™m tr·ªçng"""
        if not wind_speed:
            return 'Trung b√¨nh' if disaster_type in ['l≈© l·ª•t', 's·∫°t l·ªü'] else 'Kh√¥ng x√°c ƒë·ªãnh'
        
        match = re.search(r'c·∫•p\s*(\d+)', wind_speed)
        if match:
            level = int(match.group(1))
            if level >= 12:
                return 'R·∫•t nghi√™m tr·ªçng'
            elif level >= 10:
                return 'Nghi√™m tr·ªçng'
            elif level >= 8:
                return 'Trung b√¨nh'
            else:
                return 'Nh·∫π'
        return 'Kh√¥ng x√°c ƒë·ªãnh'

extractor = DataExtractor()

# ==================== DATA PROCESSING ====================
def process_single_article(row: pd.Series) -> Dict:
    """X·ª≠ l√Ω m·ªôt article v·ªõi error handling"""
    try:
        content = str(row.get('content', ''))
        disaster_type = str(row.get('disaster_type', ''))
        
        return {
            'location': extractor.extract_location(content),
            'wind_speed': extractor.extract_wind_speed(content),
            'rainfall': extractor.extract_rainfall(content),
            'casualties': extractor.extract_casualties(content),
            'damages': extractor.extract_damages(content),
            'event_name': extractor.extract_event_name(content),
            'severity_level': None  # Will be calculated after
        }
    except Exception as e:
        logger.warning(f"Error processing article: {e}")
        return {k: None for k in ['location', 'wind_speed', 'rainfall', 
                                   'casualties', 'damages', 'event_name', 'severity_level']}

def parallel_process_articles(df: pd.DataFrame, n_workers: int = None) -> pd.DataFrame:
    """X·ª≠ l√Ω song song v·ªõi multiprocessing"""
    if n_workers is None:
        n_workers = max(1, mp.cpu_count() - 1)
    
    logger.info(f"Processing {len(df)} articles with {n_workers} workers...")
    
    with mp.Pool(n_workers) as pool:
        results = list(tqdm(
            pool.imap(process_single_article, [row for _, row in df.iterrows()]),
            total=len(df),
            desc="Processing articles"
        ))
    
    # Merge results back to dataframe
    result_df = pd.DataFrame(results)
    for col in result_df.columns:
        df[col] = result_df[col].values
    
    # Calculate severity
    df['severity_level'] = df.apply(
        lambda row: extractor.calculate_severity(row['wind_speed'], row.get('disaster_type', '')),
        axis=1
    )
    
    return df

def filter_relevant_articles(df: pd.DataFrame) -> pd.DataFrame:
    """L·ªçc b√†i vi·∫øt li√™n quan v·ªõi improved logic"""
    disaster_keywords = [
        'b√£o', '√°p th·∫•p nhi·ªát ƒë·ªõi', 'l≈©', 'l≈© qu√©t', 'ng·∫≠p', 'h·∫°n h√°n',
        'ƒë·ªông ƒë·∫•t', 's√≥ng th·∫ßn', 'n√∫i l·ª≠a', 's·∫°t l·ªü', 'tr∆∞·ª£t', 'ch√°y r·ª´ng',
        '√¥ nhi·ªÖm', 'd·ªãch', 'r√©t', 'n·∫Øng n√≥ng', 'm∆∞a l·ªõn', 'l·ªëc'
    ]
    
    def is_relevant(row):
        text = f"{row.get('title', '')} {row.get('content', '')} {row.get('disaster_type', '')}".lower()
        return any(kw in text for kw in disaster_keywords)
    
    initial = len(df)
    df_filtered = df[df.apply(is_relevant, axis=1)].copy()
    
    # Filter by date (2020+)
    if 'date' in df_filtered.columns:
        df_filtered['date'] = pd.to_datetime(df_filtered['date'], errors='coerce')
        df_filtered = df_filtered[df_filtered['date'].dt.year >= 2020]
    
    logger.info(f"Filtered: {initial} ‚Üí {len(df_filtered)} articles")
    return df_filtered

def clean_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """L√†m s·∫°ch duplicates"""
    initial = len(df)
    df = df.drop_duplicates(subset=['url'], keep='first')
    logger.info(f"Removed {initial - len(df)} duplicate URLs")
    return df

def generate_statistics(df: pd.DataFrame):
    """Generate comprehensive statistics"""
    print("\n" + "="*70)
    print("DATA QUALITY REPORT".center(70))
    print("="*70)
    
    print(f"\nüìä Total Articles: {len(df)}")
    
    print("\nüéØ Field Completion Rates:")
    for col in ['event_name', 'location', 'wind_speed', 'rainfall', 'casualties', 'damages']:
        if col in df.columns:
            filled = df[col].notna().sum()
            rate = (filled / len(df)) * 100
            bar = "‚ñà" * int(rate / 2) + "‚ñë" * (50 - int(rate / 2))
            print(f"  {col:20s} [{bar}] {rate:5.1f}% ({filled}/{len(df)})")
    
    print("\nüìÖ Date Distribution:")
    if 'date' in df.columns:
        df['year'] = pd.to_datetime(df['date'], errors='coerce').dt.year
        year_counts = df['year'].value_counts().sort_index()
        for year, count in year_counts.items():
            if pd.notna(year):
                print(f"  {int(year)}: {count} articles")
    
    print("\nüå™Ô∏è  Disaster Type Distribution:")
    if 'disaster_type' in df.columns:
        type_counts = df['disaster_type'].value_counts().head(10)
        for dtype, count in type_counts.items():
            print(f"  {dtype:30s}: {count}")
    
    print("\n‚ö†Ô∏è  Severity Level Distribution:")
    if 'severity_level' in df.columns:
        severity_counts = df['severity_level'].value_counts()
        for level, count in severity_counts.items():
            print(f"  {level:30s}: {count}")
    
    print("\n" + "="*70)

# ==================== MAIN FUNCTION ====================
def main():
    logger.info("="*70)
    logger.info("ENHANCED DISASTER DATA PROCESSING SYSTEM v2.0")
    logger.info("="*70)
    
    # Find latest JSON file
    json_files = glob.glob('data/disaster_data_multisource_*.json')
    if not json_files:
        logger.error("‚ùå No JSON file found!")
        return
    
    json_file = max(json_files)
    logger.info(f"üìÇ Processing: {json_file}")
    
    # Load data
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    df = pd.DataFrame(data)
    logger.info(f"‚úÖ Loaded {len(df)} articles")
    
    # Filter and clean
    df = filter_relevant_articles(df)
    df = clean_duplicates(df)
    
    # Process articles (with parallel processing)
    df = parallel_process_articles(df, n_workers=4)
    
    # Add computed columns
    df['impact_area'] = df.apply(
        lambda row: row['location'] if row['location'] else 'Kh√¥ng x√°c ƒë·ªãnh',
        axis=1
    )
    
    df['damages_normalized'] = df['damages'].apply(
        lambda x: float(re.search(r'(\d+(?:\.\d+)?)', str(x)).group(1)) 
        if x and re.search(r'(\d+(?:\.\d+)?)', str(x)) else None
    )
    
    # Select columns
    columns = [
        'date', 'disaster_type', 'event_name', 'location', 'impact_area',
        'severity_level', 'title', 'source', 'category',
        'wind_speed', 'rainfall', 'casualties', 'damages', 'damages_normalized',
        'url', 'scrape_time'
    ]
    
    df = df[[col for col in columns if col in df.columns]]
    
    # Export
    output_file = 'data/disaster_data_enhanced_v2.csv'
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    logger.info(f"‚úÖ Exported to: {output_file}")
    
    # Generate statistics
    generate_statistics(df)
    
    # Sample validation
    print("\nüìã Sample Data (First 5 rows):")
    sample_cols = ['title', 'event_name', 'location', 'wind_speed', 'casualties']
    print(df[[col for col in sample_cols if col in df.columns]].head().to_string(index=False))
    
    logger.info("\n‚úÖ Processing completed successfully!")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"‚ùå Process failed: {e}", exc_info=True)
        raise
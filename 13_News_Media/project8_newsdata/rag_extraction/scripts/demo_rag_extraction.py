"""
Demo Script for RAG-Based Disaster Information Extraction

This script demonstrates the Retrieval-Augmented Generation (RAG) system
for disaster information extraction using vector databases.
"""

import json
import time
from pathlib import Path
from typing import List, Dict, Any

from scripts.rag_extractor import RAGDisasterExtractor, create_rag_extractor


def load_sample_documents() -> List[Dict[str, Any]]:
    """Load sample disaster news documents for testing"""
    sample_docs = [
        {
            "id": "doc_001",
            "content": """
            B√£o s·ªë 12 g√¢y thi·ªát h·∫°i n·∫∑ng n·ªÅ t·∫°i c√°c t·ªânh mi·ªÅn Trung. Theo b√°o c√°o s∆° b·ªô t·ª´ Ban Ch·ªâ huy
            Ph√≤ng ch·ªëng thi√™n tai Trung ∆∞∆°ng, c∆°n b√£o ƒë√£ khi·∫øn 15 ng∆∞·ªùi thi·ªát m·∫°ng, 27 ng∆∞·ªùi b·ªã th∆∞∆°ng
            v√† 5 ng∆∞·ªùi m·∫•t t√≠ch. Thi·ªát h·∫°i v·ªÅ v·∫≠t ch·∫•t ∆∞·ªõc t√≠nh kho·∫£ng 1.200 t·ª∑ ƒë·ªìng, v·ªõi 150 cƒÉn nh√†
            b·ªã s·∫≠p ho√†n to√†n v√† h√†ng trƒÉm hecta l√∫a b·ªã ng·∫≠p √∫ng.

            T·∫°i t·ªânh Qu·∫£ng Nam, b√£o s·ªë 12 ƒë·ªï b·ªô v√†o l√∫c 14h30 ng√†y 15/11/2023, g√¢y m∆∞a l·ªõn li√™n t·ª•c
            trong 3 ng√†y. ƒê·ªôi c·ª©u h·ªô ƒë√£ tri·ªÉn khai ·ª©ng c·ª©u kh·∫©n c·∫•p t·∫°i c√°c khu v·ª±c b·ªã ·∫£nh h∆∞·ªüng
            n·∫∑ng nh·∫•t. Qu√¢n ƒë·ªôi v√† H·ªôi Ch·ªØ th·∫≠p ƒë·ªè ƒë√£ huy ƒë·ªông h√†ng trƒÉm c√°n b·ªô, chi·∫øn sƒ© tham gia
            c·ª©u h·ªô, t√¨m ki·∫øm ng∆∞·ªùi m·∫•t t√≠ch.
            """,
            "metadata": {
                "source": "Vietnam News",
                "date": "2023-11-16",
                "location": "Qu·∫£ng Nam",
                "disaster_type": "B√£o"
            }
        },
        {
            "id": "doc_002",
            "content": """
            L≈© qu√©t x·∫£y ra t·∫°i huy·ªán M∆∞·ªùng Kh∆∞∆°ng, t·ªânh L√†o Cai v√†o s√°ng ng√†y 20/10/2023.
            Theo th√¥ng tin t·ª´ ·ª¶y ban nh√¢n d√¢n huy·ªán, tr·∫≠n l≈© ƒë√£ g√¢y thi·ªát h·∫°i nghi√™m tr·ªçng v·ªõi
            8 ng∆∞·ªùi ch·∫øt, 12 ng∆∞·ªùi m·∫•t t√≠ch v√† h√†ng ch·ª•c ng√¥i nh√† b·ªã cu·ªën tr√¥i. Thi·ªát h·∫°i kinh t·∫ø
            ban ƒë·∫ßu ∆∞·ªõc t√≠nh 50 t·ª∑ ƒë·ªìng.

            Nguy√™n nh√¢n ban ƒë·∫ßu ƒë∆∞·ª£c x√°c ƒë·ªãnh l√† do m∆∞a l·ªõn k√©o d√†i nhi·ªÅu ng√†y, khi·∫øn ƒë·∫•t ƒë√° t·ª´
            c√°c qu·∫£ ƒë·ªìi cao b·ªã s·∫°t l·ªü, t·∫°o th√†nh d√≤ng l≈© qu√©t v·ªõi t·ªëc ƒë·ªô r·∫•t nhanh. Qu√¢n ƒë·ªôi v√†
            H·ªôi Ch·ªØ th·∫≠p ƒë·ªè ƒë√£ huy ƒë·ªông l·ª±c l∆∞·ª£ng c·ª©u h·ªô, t√¨m ki·∫øm ng∆∞·ªùi m·∫•t t√≠ch t·∫°i khu v·ª±c
            x·∫£y ra l≈©. C√¥ng t√°c c·ª©u h·ªô ƒëang g·∫∑p nhi·ªÅu kh√≥ khƒÉn do ƒë·ªãa h√¨nh hi·ªÉm tr·ªü.
            """,
            "metadata": {
                "source": "Lao Cai News",
                "date": "2023-10-21",
                "location": "L√†o Cai",
                "disaster_type": "L≈© qu√©t"
            }
        },
        {
            "id": "doc_003",
            "content": """
            ƒê·ªông ƒë·∫•t m·∫°nh 6.5 ƒë·ªô Richter x·∫£y ra t·∫°i huy·ªán S√¨n H·ªì, t·ªânh Lai Ch√¢u v√†o l√∫c 22h45
            ng√†y 18/6/2024. Theo Trung t√¢m B√°o tin ƒë·ªông ƒë·∫•t v√† C·∫£nh b√°o s√≥ng th·∫ßn, t√¢m ch·∫•n n·∫±m
            ·ªü ƒë·ªô s√¢u 10km v·ªõi b√°n k√≠nh ·∫£nh h∆∞·ªüng 50km. Hi·ªán ch∆∞a c√≥ th√¥ng tin v·ªÅ thi·ªát h·∫°i v·ªÅ
            ng∆∞·ªùi v√† t√†i s·∫£n.

            C√°c c∆° quan ch·ª©c nƒÉng ƒëang ki·ªÉm tra, ƒë√°nh gi√° m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng c·ªßa tr·∫≠n ƒë·ªông ƒë·∫•t.
            Ng∆∞·ªùi d√¢n t·∫°i khu v·ª±c t√¢m ch·∫•n c·∫£m nh·∫≠n ƒë∆∞·ª£c rung ƒë·ªông m·∫°nh, m·ªôt s·ªë ƒë·ªì ƒë·∫°c trong
            nh√† b·ªã r∆°i v·ª°. Kh√¥ng c√≥ c·∫£nh b√°o s√≥ng th·∫ßn v√¨ t√¢m ch·∫•n n·∫±m s√¢u d∆∞·ªõi ƒë·∫•t li·ªÅn.
            """,
            "metadata": {
                "source": "Lai Chau News",
                "date": "2024-06-19",
                "location": "Lai Ch√¢u",
                "disaster_type": "ƒê·ªông ƒë·∫•t"
            }
        },
        {
            "id": "doc_004",
            "content": """
            H·∫°n h√°n k√©o d√†i t·∫°i c√°c t·ªânh T√¢y Nguy√™n khi·∫øn h√†ng ngh√¨n hecta c√† ph√™ b·ªã kh√¥ h·∫°n.
            Theo S·ªü N√¥ng nghi·ªáp t·ªânh ƒê·∫Øk L·∫Øk, h·∫°n h√°n nƒÉm 2024 nghi√™m tr·ªçng h∆°n m·ªçi nƒÉm,
            ·∫£nh h∆∞·ªüng ƒë·∫øn 50.000 h·ªô d√¢n tr·ªìng c√† ph√™. N·∫Øng n√≥ng k√©o d√†i t·ª´ ƒë·∫ßu nƒÉm khi·∫øn
            c√°c h·ªì ch·ª©a n∆∞·ªõc c·∫°n ki·ªát, s√¥ng su·ªëi kh√¥ h·∫°n.

            UBND t·ªânh ƒê·∫Øk L·∫Øk ƒë√£ ch·ªâ ƒë·∫°o c√°c ƒë·ªãa ph∆∞∆°ng tri·ªÉn khai c√°c bi·ªán ph√°p ·ª©ng ph√≥
            v·ªõi h·∫°n h√°n nh∆∞ khoan gi·∫øng, ƒë√†o ao tr·ªØ n∆∞·ªõc, chuy·ªÉn ƒë·ªïi c∆° c·∫•u c√¢y tr·ªìng.
            D·ª± b√°o h·∫°n h√°n s·∫Ω c√≤n k√©o d√†i ƒë·∫øn m√πa m∆∞a nƒÉm 2024.
            """,
            "metadata": {
                "source": "Dak Lak News",
                "date": "2024-04-15",
                "location": "ƒê·∫Øk L·∫Øk",
                "disaster_type": "H·∫°n h√°n"
            }
        }
    ]

    return sample_docs


def demo_document_ingestion():
    """Demonstrate document ingestion into vector database"""
    print("=" * 70)
    print("RAG DOCUMENT INGESTION DEMO")
    print("=" * 70)

    try:
        # Create RAG extractor
        extractor = create_rag_extractor(vector_db="chroma", embedding="sentence-transformers")
        print("‚úÖ RAG extractor initialized")

        # Load sample documents
        documents = load_sample_documents()
        print(f"üìÑ Loaded {len(documents)} sample documents")

        # Clear existing data
        print("üßπ Clearing existing database...")
        extractor.clear_database()

        # Add documents
        print("üì• Adding documents to vector database...")
        start_time = time.time()
        success = extractor.add_documents(documents)
        ingestion_time = time.time() - start_time

        if success:
            print(".2f"            print(f"üìä Total chunks created: {extractor.metrics['total_chunks']}")
        else:
            print("‚ùå Document ingestion failed")
            return False

        return True

    except Exception as e:
        print(f"‚ùå Ingestion demo failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def demo_similarity_search():
    """Demonstrate similarity search in vector database"""
    print("\n" + "=" * 70)
    print("VECTOR SIMILARITY SEARCH DEMO")
    print("=" * 70)

    try:
        extractor = create_rag_extractor()

        # Test queries
        test_queries = [
            "th√¥ng tin b√£o t·∫°i Qu·∫£ng Nam",
            "l≈© qu√©t ·ªü L√†o Cai",
            "ƒë·ªông ƒë·∫•t Lai Ch√¢u",
            "h·∫°n h√°n T√¢y Nguy√™n",
            "thi√™n tai mi·ªÅn n√∫i"
        ]

        for query in test_queries:
            print(f"\nüîç Query: '{query}'")
            print("-" * 40)

            # Search for relevant chunks
            results = extractor.search_documents(query, top_k=3)

            if results:
                for i, result in enumerate(results, 1):
                    score = result.get('score', 0.0)
                    preview = result.get('text', '')[:150] + '...'
                    metadata = result.get('metadata', {})

                    print(f"  {i}. Score: {score:.3f}")
                    print(f"     Preview: {preview}")
                    print(f"     Source: {metadata.get('source', 'Unknown')}")
                    print(f"     Date: {metadata.get('date', 'Unknown')}")
            else:
                print("  ‚ùå No results found")

        return True

    except Exception as e:
        print(f"‚ùå Search demo failed: {e}")
        return False


def demo_rag_extraction():
    """Demonstrate full RAG extraction pipeline"""
    print("\n" + "=" * 70)
    print("RAG-BASED EXTRACTION DEMO")
    print("=" * 70)

    try:
        extractor = create_rag_extractor()

        # Test extraction queries
        extraction_queries = [
            "Thi·ªát h·∫°i do b√£o s·ªë 12 t·∫°i Qu·∫£ng Nam",
            "Th√¥ng tin l≈© qu√©t M∆∞·ªùng Kh∆∞∆°ng L√†o Cai",
            "ƒê·ªông ƒë·∫•t t·∫°i S√¨n H·ªì Lai Ch√¢u",
            "T√¨nh h√¨nh h·∫°n h√°n ·ªü ƒê·∫Øk L·∫Øk",
            "Thi√™n tai g√¢y thi·ªát h·∫°i l·ªõn nh·∫•t nƒÉm 2023"
        ]

        for query in extraction_queries:
            print(f"\nü§ñ Extracting: '{query}'")
            print("-" * 50)

            # Perform RAG extraction
            start_time = time.time()
            result = extractor.extract_disaster_info(query)
            extraction_time = time.time() - start_time

            if result:
                print(".2f"                print(f"üí∞ Cost: ${result.cost_estimate:.4f}")
                print(f"üéØ Confidence: {result.confidence_score:.2f}")
                print()

                # Display extracted information
                info = result.extracted_info
                if "error" not in info:
                    print("üìã EXTRACTED INFORMATION:")
                    for key, value in info.items():
                        if value and value != "N/A":
                            print(f"  ‚Ä¢ {key}: {value}")
                else:
                    print(f"‚ùå Error: {info.get('error', 'Unknown error')}")
            else:
                print("‚ùå Extraction failed or no relevant information found")

            print("-" * 50)

        return True

    except Exception as e:
        print(f"‚ùå RAG extraction demo failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def demo_metrics_and_performance():
    """Demonstrate system metrics and performance"""
    print("\n" + "=" * 70)
    print("SYSTEM METRICS & PERFORMANCE DEMO")
    print("=" * 70)

    try:
        extractor = create_rag_extractor()

        # Get system metrics
        metrics = extractor.get_metrics()

        print("üìä SYSTEM METRICS:")
        print("-" * 40)
        print(f"Vector DB Type: {metrics['vector_db_type']}")
        print(f"Embedding Model: {metrics['embedding_model']}")
        print(f"Chunking Strategy: {metrics['chunking_strategy']}")
        print(f"Total Documents: {metrics['total_documents']}")
        print(f"Total Chunks: {metrics['total_chunks']}")
        print(f"Total Queries: {metrics['total_queries']}")
        print(f"Cache Hits: {metrics['cache_hits']}")
        print(".2f"
        # Test performance with multiple queries
        print("
‚è±Ô∏è  PERFORMANCE TEST:"        print("-" * 40)

        test_queries = [
            "b√£o l≈© mi·ªÅn Trung",
            "thi√™n tai T√¢y Nguy√™n",
            "ƒë·ªông ƒë·∫•t mi·ªÅn n√∫i"
        ]

        total_time = 0
        for query in test_queries:
            start_time = time.time()
            results = extractor.search_documents(query, top_k=5)
            query_time = time.time() - start_time
            total_time += query_time
            print(".3f"
        avg_time = total_time / len(test_queries)
        print(".3f"
        return True

    except Exception as e:
        print(f"‚ùå Metrics demo failed: {e}")
        return False


def demo_batch_processing():
    """Demonstrate batch document processing"""
    print("\n" + "=" * 70)
    print("BATCH PROCESSING DEMO")
    print("=" * 70)

    try:
        extractor = create_rag_extractor()

        # Create larger batch of documents
        base_docs = load_sample_documents()
        batch_docs = []

        # Create variations of documents
        for i, doc in enumerate(base_docs):
            for j in range(3):  # Create 3 variations each
                new_doc = doc.copy()
                new_doc["id"] = f"{doc['id']}_var_{j}"
                new_doc["content"] = doc["content"] + f"\n\n[Phi√™n b·∫£n {j+1}]"
                new_doc["metadata"] = {**doc["metadata"], "variation": j+1}
                batch_docs.append(new_doc)

        print(f"üìÑ Created batch of {len(batch_docs)} documents")

        # Clear database
        extractor.clear_database()

        # Batch add documents
        print("üì• Adding documents in batch...")
        start_time = time.time()
        success = extractor.add_documents(batch_docs)
        batch_time = time.time() - start_time

        if success:
            print(".2f"            print(f"üìä Documents processed: {len(batch_docs)}")
            print(f"üìä Chunks created: {extractor.metrics['total_chunks']}")
            print(".1f"
        else:
            print("‚ùå Batch processing failed")

        return success

    except Exception as e:
        print(f"‚ùå Batch demo failed: {e}")
        return False


def run_full_demo():
    """Run complete RAG system demonstration"""
    print("üöÄ STARTING COMPLETE RAG SYSTEM DEMO")
    print("This demo showcases the full RAG pipeline for disaster information extraction")
    print()

    demo_results = []

    # Run all demos
    demos = [
        ("Document Ingestion", demo_document_ingestion),
        ("Similarity Search", demo_similarity_search),
        ("RAG Extraction", demo_rag_extraction),
        ("Batch Processing", demo_batch_processing),
        ("Metrics & Performance", demo_metrics_and_performance)
    ]

    for demo_name, demo_func in demos:
        print(f"\n{'='*20} {demo_name.upper()} {'='*20}")
        try:
            result = demo_func()
            demo_results.append((demo_name, result))
            status = "‚úÖ PASSED" if result else "‚ùå FAILED"
            print(f"\n{status}: {demo_name}")
        except Exception as e:
            print(f"\n‚ùå FAILED: {demo_name} - {e}")
            demo_results.append((demo_name, False))

    # Summary
    print("\n" + "=" * 70)
    print("DEMO SUMMARY")
    print("=" * 70)

    passed = sum(1 for _, result in demo_results if result)
    total = len(demo_results)

    print(f"Total Demos: {total}")
    print(f"Passed: {passed}")
    print(f"Failed: {total - passed}")

    for demo_name, result in demo_results:
        status = "‚úÖ" if result else "‚ùå"
        print(f"  {status} {demo_name}")

    if passed == total:
        print("\nüéâ ALL DEMOS PASSED! RAG system is working correctly.")
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} demo(s) failed. Check the errors above.")

    print("=" * 70)


def main():
    """Main demo function"""
    try:
        run_full_demo()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Demo interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Demo failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Demo Keyword-based Disaster Information Extraction
Demo Ä‘áº§y Ä‘á»§ cho há»‡ thá»‘ng trÃ­ch xuáº¥t thÃ´ng tin thiÃªn tai dá»±a trÃªn tá»« khÃ³a
Uses real data from disaster_data_multisource_20251207_165113.json
"""

import os
import sys
from datetime import datetime
from pathlib import Path

# Add parent directory to path
sys.path.append(os.path.dirname(__file__))

from keyword_extractor import KeywordExtractor, save_results_to_csv, save_results_to_json


def load_sample_articles():
    """Load real disaster data from JSON file"""
    data_file = os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'disaster_data_multisource_20251207_165113.json')
    if os.path.exists(data_file):
        import json
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        articles = []
        for item in data:  # Load all articles
            article = {
                'title': item.get('title', ''),
                'content': item.get('content', ''),
                'url': item.get('url', ''),
                'source': item.get('source', '')
            }
            articles.append(article)
        return articles
    else:
        print("âš ï¸ Could not load real data file")
        print("Using fallback sample data...")
        return load_sample_articles_fallback()

def load_sample_articles_fallback():
    """Táº¡o dá»¯ liá»‡u máº«u Ä‘á»ƒ demo (fallback)"""
    sample_articles = [
        {
            'title': 'BÃ£o sá»‘ 9 gÃ¢y thiá»‡t háº¡i náº·ng táº¡i cÃ¡c tá»‰nh miá»n Trung',
            'content': '''BÃ£o sá»‘ 9 Ä‘Ã£ Ä‘á»• bá»™ vÃ o cÃ¡c tá»‰nh miá»n Trung vÃ o sÃ¡ng nay. GiÃ³ máº¡nh cáº¥p 12-13, sÃ³ng biá»ƒn cao 5-7m. HÃ ng trÄƒm ngÃ´i nhÃ  bá»‹ tá»‘c mÃ¡i, nhiá»u diá»‡n tÃ­ch lÃºa bá»‹ ngáº­p Ãºng. CÃ³ 3 ngÆ°á»i cháº¿t, 10 ngÆ°á»i bá»‹ thÆ°Æ¡ng. Thiá»‡t háº¡i ban Ä‘áº§u Æ°á»›c tÃ­nh hÃ ng trÄƒm tá»· Ä‘á»“ng.''',
            'url': 'https://vnexpress.net/bao-so-9',
            'source': 'vnexpress'
        },
        {
            'title': 'Äá»™ng Ä‘áº¥t máº¡nh 6.5 Ä‘á»™ Richter táº¡i Kon Tum',
            'content': '''SÃ¡ng nay xáº£y ra Ä‘á»™ng Ä‘áº¥t máº¡nh 6.5 Ä‘á»™ Richter táº¡i huyá»‡n Kon PlÃ´ng, tá»‰nh Kon Tum. Trung tÃ¢m bÃ¡o tin Ä‘á»™ng Ä‘áº¥t ghi nháº­n tráº­n Ä‘á»™ng Ä‘áº¥t xáº£y ra vÃ o lÃºc 7 giá» 45 phÃºt. Rung cháº¥n kÃ©o dÃ i khoáº£ng 30 giÃ¢y.''',
            'url': 'https://dantri.com.vn/dong-dat-kon-tum',
            'source': 'dantri'
        }
    ]
    return sample_articles


def run_demo():
    """Cháº¡y demo keyword extraction"""
    print("ğŸš€ Demo Keyword-based Disaster Information Extraction")
    print("=" * 60)

    # Khá»Ÿi táº¡o extractor
    print("ğŸ“‹ Khá»Ÿi táº¡o KeywordExtractor...")
    extractor = KeywordExtractor()

    # Load sample data
    print("ğŸ“š Load dá»¯ liá»‡u máº«u...")
    sample_articles = load_sample_articles()
    print(f"   TÃ¬m tháº¥y {len(sample_articles)} bÃ i bÃ¡o máº«u")

    # Process batch
    print("\nğŸ” Äang xá»­ lÃ½ batch...")
    start_time = datetime.now()
    results = extractor.process_batch(sample_articles)
    end_time = datetime.now()

    processing_time = (end_time - start_time).total_seconds()

    # Hiá»ƒn thá»‹ káº¿t quáº£ tá»•ng quan
    print("\nğŸ“Š Káº¾T QUáº¢ Tá»”NG QUAN:")
    print(f"   Thá»i gian xá»­ lÃ½: {processing_time:.2f} giÃ¢y")
    print(f"   Sá»‘ bÃ i bÃ¡o xá»­ lÃ½: {len(results)}")

    total_sentences = sum(r.get('summary', {}).get('total_sentences_extracted', 0)
                         for r in results if 'error' not in r)
    print(f"   Tá»•ng cÃ¢u trÃ­ch xuáº¥t: {total_sentences}")

    # Chi tiáº¿t tá»«ng bÃ i
    print("\nğŸ“ CHI TIáº¾T Tá»ªNG BÃ€I BÃO:")
    for i, result in enumerate(results, 1):
        if 'error' in result:
            print(f"   {i}. âŒ Lá»—i: {result['error']}")
            continue

        article = result['article_info']
        summary = result['summary']

        print(f"   {i}. âœ… {article['title'][:50]}...")
        print(f"      Nguá»“n: {article['source']}")
        print(f"      CÃ¢u trÃ­ch xuáº¥t: {summary['total_sentences_extracted']}")
        print(f"      Tá»« khÃ³a unique: {summary['unique_keywords']}")
        print(f"      Loáº¡i thiÃªn tai: {summary['disaster_types_detected']}")
        print(f"      Äá»™ tin cáº­y TB: {summary['avg_confidence']:.2f}")

        # Hiá»ƒn thá»‹ sample sentences
        if result['extraction_results']:
            print("      ğŸ“„ Sample cÃ¢u trÃ­ch xuáº¥t:")
            for sent in result['extraction_results'][:2]:  # Show max 2 sentences
                keywords = [kw for kw, _ in sent['keywords_found']]
                print(f"         â€¢ {sent['sentence'][:80]}...")
                print(f"           Tá»« khÃ³a: {keywords}")

    # LÆ°u káº¿t quáº£
    print("\nğŸ’¾ Äang lÆ°u káº¿t quáº£...")
    output_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
    os.makedirs(output_dir, exist_ok=True)

    csv_path = os.path.join(output_dir, 'keyword_extraction_demo.csv')
    json_path = os.path.join(output_dir, 'keyword_extraction_demo.json')

    save_results_to_csv(results, csv_path)
    save_results_to_json(results, json_path)

    print(f"   âœ… ÄÃ£ lÆ°u CSV: {csv_path}")
    print(f"   âœ… ÄÃ£ lÆ°u JSON: {json_path}")

    print("\nğŸ¯ DEMO HOÃ€N THÃ€NH!")
    print("   Báº¡n cÃ³ thá»ƒ xem káº¿t quáº£ chi tiáº¿t trong thÆ° má»¥c data/")
    print("   File CSV cÃ³ thá»ƒ má»Ÿ báº±ng Excel Ä‘á»ƒ xem dá»… dÃ ng hÆ¡n.")


if __name__ == "__main__":
    run_demo()
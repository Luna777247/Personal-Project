# AI Ops Dashboard - MLOps Mini System

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![MLflow](https://img.shields.io/badge/MLflow-2.9+-blue.svg)](https://mlflow.org/)
[![Docker](https://img.shields.io/badge/Docker-24.0+-blue.svg)](https://www.docker.com/)
[![Triton](https://img.shields.io/badge/Triton-2.40+-green.svg)](https://github.com/triton-inference-server/server)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Enterprise-grade MLOps system** cho model lifecycle management, tá»« training Ä‘áº¿n production deployment vá»›i monitoring vÃ  CI/CD automation.

## ğŸ¯ Tá»•ng Quan

**AI Ops Dashboard** lÃ  má»™t há»‡ thá»‘ng MLOps hoÃ n chá»‰nh giÃºp quáº£n lÃ½ toÃ n bá»™ vÃ²ng Ä‘á»i cá»§a Machine Learning models, tá»« training, versioning, deployment Ä‘áº¿n monitoring. Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho mÃ´i trÆ°á»ng banking/enterprise vá»›i yÃªu cáº§u cao vá» reliability, scalability vÃ  observability.

### ğŸŒŸ Táº¡i Sao Dá»± Ãn NÃ y Quan Trá»ng?

**Cho MB Bank / Enterprise**:
- âœ… Quáº£n lÃ½ hÃ ng chá»¥c/trÄƒm models trong production
- âœ… Tá»± Ä‘á»™ng hÃ³a deployment â†’ giáº£m human error
- âœ… Model versioning â†’ rollback nhanh khi cÃ³ issue
- âœ… Monitoring real-time â†’ phÃ¡t hiá»‡n model drift
- âœ… CI/CD pipeline â†’ accelerate time-to-market

**Cho Fresher**:
- ğŸš€ **Ráº¤T ÃT Fresher cÃ³ MLOps experience** â†’ ná»•i báº­t vÆ°á»£t trá»™i
- ğŸš€ ÄÃ¡p á»©ng "DevOps/MLOps" requirement trong JD
- ğŸš€ Chá»©ng minh hiá»ƒu production deployment, khÃ´ng chá»‰ training models
- ğŸš€ Show Ä‘Æ°á»£c collaboration vá»›i Data Engineers/DevOps team

## ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AI Ops Dashboard                         â”‚
â”‚                     (Streamlit/Flask UI)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MLflow     â”‚    â”‚   Triton     â”‚    â”‚  Prometheus  â”‚
â”‚   Tracking   â”‚    â”‚  Inference   â”‚    â”‚  Monitoring  â”‚
â”‚   Server     â”‚    â”‚   Server     â”‚    â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - Experimentsâ”‚    â”‚ - Model Repo â”‚    â”‚ - Metrics    â”‚
â”‚ - Metrics    â”‚    â”‚ - gRPC API   â”‚    â”‚ - Alerts     â”‚
â”‚ - Artifacts  â”‚    â”‚ - HTTP API   â”‚    â”‚ - Grafana    â”‚
â”‚ - Registry   â”‚    â”‚ - GPU Accel  â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    â”‚                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   PostgreSQL    â”‚
                    â”‚   (Backend DB)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ TÃ­nh NÄƒng ChÃ­nh

### 1. ğŸ“Š MLflow Tracking & Registry

**Training Experiment Tracking**:
- Tá»± Ä‘á»™ng log metrics (accuracy, loss, F1, AUC)
- Log hyperparameters (learning rate, batch size, epochs)
- Save model artifacts (weights, configs, preprocessing)
- Compare experiments vá»›i UI trá»±c quan
- Search/filter experiments theo tags

**Model Registry**:
- Version control cho models (v1, v2, v3...)
- Stage management: `None` â†’ `Staging` â†’ `Production` â†’ `Archived`
- Model lineage tracking (data source, code version, training config)
- Model signatures (input/output schema validation)
- Approval workflow cho production deployment

**VÃ­ Dá»¥ Code**:
```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier

# Start tracking
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("fraud-detection-v2")

with mlflow.start_run(run_name="rf-tuned-001"):
    # Log parameters
    mlflow.log_param("n_estimators", 100)
    mlflow.log_param("max_depth", 10)
    
    # Train model
    model = RandomForestClassifier(n_estimators=100, max_depth=10)
    model.fit(X_train, y_train)
    
    # Log metrics
    accuracy = model.score(X_test, y_test)
    mlflow.log_metric("accuracy", accuracy)
    
    # Log model
    mlflow.sklearn.log_model(model, "model", 
                             registered_model_name="fraud-detector")
```

### 2. ğŸš€ Triton Inference Server

**High-Performance Serving**:
- Há»— trá»£ multiple frameworks (TensorFlow, PyTorch, ONNX, Scikit-learn)
- Dynamic batching â†’ tÄƒng throughput 10-100x
- Model ensemble â†’ combine multiple models
- GPU acceleration vá»›i CUDA
- Concurrent model execution

**API Protocols**:
- **gRPC**: Low-latency inference (< 10ms overhead)
- **HTTP/REST**: Easy integration
- **Model metadata**: Query model info, config

**Model Repository Structure**:
```
models/
â”œâ”€â”€ fraud_detector/
â”‚   â”œâ”€â”€ config.pbtxt          # Model configuration
â”‚   â””â”€â”€ 1/                    # Version 1
â”‚       â””â”€â”€ model.onnx
â”œâ”€â”€ credit_scorer/
â”‚   â”œâ”€â”€ config.pbtxt
â”‚   â””â”€â”€ 1/
â”‚       â””â”€â”€ model.pkl
```

**gRPC Inference Example**:
```python
import tritonclient.grpc as grpcclient
import numpy as np

# Connect to Triton
client = grpcclient.InferenceServerClient(url="localhost:8001")

# Prepare input
input_data = np.array([[0.5, 0.3, 0.8, 0.2]], dtype=np.float32)
inputs = [grpcclient.InferInput("INPUT0", input_data.shape, "FP32")]
inputs[0].set_data_from_numpy(input_data)

# Infer
outputs = [grpcclient.InferRequestedOutput("OUTPUT0")]
response = client.infer(model_name="fraud_detector", inputs=inputs, outputs=outputs)

# Get result
result = response.as_numpy("OUTPUT0")
print(f"Prediction: {result}")
```

### 3. ğŸ”„ CI/CD Pipeline (GitHub Actions)

**Automated Workflow**:
```yaml
Training â†’ Testing â†’ Build Docker â†’ Deploy to Staging â†’ 
Manual Approval â†’ Deploy to Production â†’ Monitor
```

**Pipeline Stages**:

1. **Code Quality**:
   - Linting (flake8, black)
   - Type checking (mypy)
   - Security scan (bandit)

2. **Model Training**:
   - Train on latest data
   - Log to MLflow
   - Validate metrics thresholds

3. **Model Testing**:
   - Unit tests (pytest)
   - Integration tests
   - Performance tests
   - Bias/fairness tests

4. **Docker Build**:
   - Build training image
   - Build inference image
   - Push to registry (Docker Hub/ECR)

5. **Deployment**:
   - Deploy to staging
   - Run smoke tests
   - Deploy to production (manual approval)

6. **Monitoring**:
   - Alert on deployment
   - Track metrics
   - Log model version

### 4. ğŸ“ˆ Monitoring & Observability

**Prometheus Metrics**:
- Request rate (req/s)
- Latency (p50, p95, p99)
- Error rate (%)
- Model drift score
- GPU utilization

**Grafana Dashboards**:
- Real-time inference metrics
- Model performance trends
- Resource utilization
- Alert history

**Model Monitoring**:
- Data drift detection (KS test, PSI)
- Prediction drift (distribution shift)
- Performance degradation alerts
- Automatic retraining triggers

### 5. ğŸ¨ Dashboard UI

**Streamlit Dashboard Features**:
- ğŸ“Š Experiment comparison view
- ğŸ“ˆ Model performance charts
- ğŸš€ One-click model deployment
- ğŸ“‹ Model registry browser
- ğŸ” Inference testing tool
- âš™ï¸ Configuration management

## ğŸ“¦ CÃ i Äáº·t

### Prerequisites

```bash
# Required
- Docker 24.0+
- Docker Compose 2.0+
- Python 3.9+
- NVIDIA GPU (optional, for Triton GPU acceleration)
- Git 2.0+
```

### Quick Start (5 phÃºt)

```bash
# 1. Clone repository
git clone https://github.com/your-username/mlops-dashboard.git
cd mlops-dashboard

# 2. Start all services with Docker Compose
docker-compose up -d

# 3. Verify services
docker-compose ps

# Services started:
# - MLflow UI: http://localhost:5000
# - Triton Server: localhost:8000 (HTTP), localhost:8001 (gRPC)
# - Prometheus: http://localhost:9090
# - Grafana: http://localhost:3000 (admin/admin)
# - Dashboard: http://localhost:8501
```

### Development Setup

```bash
# 1. Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Setup environment variables
cp .env.example .env
# Edit .env with your configurations

# 4. Initialize database
python scripts/init_db.py

# 5. Run training example
python examples/train_model.py
```

## ğŸš€ Sá»­ Dá»¥ng

### 1. Train Model vá»›i MLflow Tracking

```python
# examples/train_fraud_detector.py
import mlflow
from src.training.trainer import FraudDetectorTrainer

# Configure MLflow
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("fraud-detection")

# Initialize trainer
trainer = FraudDetectorTrainer(
    model_type="random_forest",
    hyperparameters={
        "n_estimators": 100,
        "max_depth": 10,
        "min_samples_split": 5
    }
)

# Train with auto-logging
with mlflow.start_run():
    trainer.train(X_train, y_train)
    metrics = trainer.evaluate(X_test, y_test)
    
    # Model Ä‘Æ°á»£c tá»± Ä‘á»™ng log vÃ o MLflow
    print(f"Model accuracy: {metrics['accuracy']:.4f}")
    print(f"Run ID: {mlflow.active_run().info.run_id}")
```

### 2. Register Model to Production

```python
# examples/register_model.py
import mlflow
from mlflow.tracking import MlflowClient

client = MlflowClient()

# Get best run from experiment
experiment = client.get_experiment_by_name("fraud-detection")
runs = client.search_runs(
    experiment_ids=[experiment.experiment_id],
    order_by=["metrics.f1_score DESC"],
    max_results=1
)

best_run = runs[0]

# Register model
model_uri = f"runs:/{best_run.info.run_id}/model"
mv = mlflow.register_model(model_uri, "fraud-detector")

# Promote to production
client.transition_model_version_stage(
    name="fraud-detector",
    version=mv.version,
    stage="Production"
)

print(f"Model v{mv.version} promoted to Production!")
```

### 3. Deploy to Triton Server

```bash
# Export model to Triton format
python scripts/export_to_triton.py \
  --model-name fraud-detector \
  --version 1 \
  --format onnx

# Model Ä‘Æ°á»£c export vÃ o models/fraud_detector/1/

# Triton tá»± Ä‘á»™ng load model má»›i (hot reload)
# Verify deployment
curl http://localhost:8000/v2/models/fraud_detector
```

### 4. Inference qua gRPC

```python
# examples/inference_grpc.py
from src.inference.triton_client import TritonGRPCClient
import numpy as np

# Connect to Triton
client = TritonGRPCClient(url="localhost:8001")

# Prepare transaction features
transaction = np.array([[
    1000.0,    # amount
    0.5,       # hour_of_day_normalized
    1,         # is_international
    2.3,       # velocity_score
    0.1        # merchant_risk_score
]], dtype=np.float32)

# Infer
result = client.infer(
    model_name="fraud_detector",
    inputs={"INPUT": transaction}
)

is_fraud = result["OUTPUT"][0] > 0.5
confidence = result["OUTPUT"][0]

print(f"Fraud: {is_fraud}, Confidence: {confidence:.2%}")
```

### 5. Monitor vá»›i Prometheus

```python
# src/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# Define metrics
inference_counter = Counter(
    'model_inference_total',
    'Total inference requests',
    ['model', 'version', 'status']
)

inference_latency = Histogram(
    'model_inference_latency_seconds',
    'Inference latency',
    ['model', 'version']
)

model_accuracy = Gauge(
    'model_accuracy',
    'Current model accuracy',
    ['model', 'version']
)

# Use in inference
with inference_latency.labels(model="fraud_detector", version="1").time():
    prediction = model.predict(features)
    
inference_counter.labels(
    model="fraud_detector",
    version="1",
    status="success"
).inc()
```

## ğŸ“Š Dashboard Features

### MLflow UI (http://localhost:5000)

- **Experiments**: View all training runs
- **Compare**: Side-by-side metrics comparison
- **Artifacts**: Download models, logs, plots
- **Models**: Registry vá»›i versioning

### Triton Metrics (http://localhost:8002/metrics)

```
# Triton built-in metrics
nv_inference_request_success{model="fraud_detector",version="1"} 1234
nv_inference_request_duration_us{model="fraud_detector",version="1"} 5432
nv_inference_queue_duration_us{model="fraud_detector",version="1"} 123
nv_gpu_utilization{gpu_uuid="GPU-..."} 45.2
```

### Custom Dashboard (http://localhost:8501)

**Pages**:
1. **Overview**: System health, active models, recent deployments
2. **Training**: Start training, view experiments, compare runs
3. **Registry**: Browse models, promote versions, view lineage
4. **Deployment**: Deploy to Triton, test inference, rollback
5. **Monitoring**: Real-time metrics, alerts, drift detection

## ğŸ”§ Configuration

### MLflow Configuration

```yaml
# config/mlflow.yaml
tracking_uri: http://localhost:5000
artifact_location: s3://mlflow-artifacts  # or local path
backend_store_uri: postgresql://user:pass@db:5432/mlflow

experiments:
  fraud_detection:
    tags:
      team: ml-team
      project: fraud-prevention
    
  credit_scoring:
    tags:
      team: risk-team
      project: credit-risk
```

### Triton Configuration

```protobuf
# models/fraud_detector/config.pbtxt
name: "fraud_detector"
platform: "onnxruntime_onnx"
max_batch_size: 128

input [
  {
    name: "INPUT"
    data_type: TYPE_FP32
    dims: [ 5 ]
  }
]

output [
  {
    name: "OUTPUT"
    data_type: TYPE_FP32
    dims: [ 1 ]
  }
]

dynamic_batching {
  preferred_batch_size: [ 16, 32 ]
  max_queue_delay_microseconds: 100
}

instance_group [
  {
    count: 2
    kind: KIND_GPU
  }
]
```

## ğŸ§ª Testing

### Run All Tests

```bash
# Unit tests
pytest tests/unit/ -v

# Integration tests
pytest tests/integration/ -v

# End-to-end tests
pytest tests/e2e/ -v

# With coverage
pytest tests/ --cov=src --cov-report=html
```

### Performance Testing

```bash
# Load test Triton Server
python tests/performance/load_test.py \
  --url localhost:8001 \
  --model fraud_detector \
  --concurrent 100 \
  --duration 60

# Expected results:
# Throughput: 1000+ req/s
# Latency p50: < 10ms
# Latency p99: < 50ms
```

## ğŸ“ˆ Performance Benchmarks

### MLflow Tracking

| Metric | Value | Notes |
|--------|-------|-------|
| Experiment logging | < 100ms | Per run |
| Artifact upload | ~1MB/s | To S3 |
| UI query | < 500ms | 1000 runs |
| Concurrent runs | 50+ | Same experiment |

### Triton Inference

| Model Type | Batch Size | Throughput | Latency (p99) |
|------------|------------|------------|---------------|
| ONNX (CPU) | 1 | 100 req/s | 15ms |
| ONNX (CPU) | 32 | 500 req/s | 80ms |
| ONNX (GPU) | 1 | 500 req/s | 5ms |
| ONNX (GPU) | 32 | 3000 req/s | 20ms |

## ğŸ¦ PhÃ¹ Há»£p vá»›i MB Bank

### 1. Enterprise MLOps Requirements

âœ… **Model Governance**:
- Model registry vá»›i approval workflow
- Version control vÃ  lineage tracking
- Audit trail cho compliance
- Role-based access control (RBAC)

âœ… **Production Deployment**:
- Automated CI/CD pipeline
- Blue-green deployment support
- Rollback capability
- Canary deployment (A/B testing)

âœ… **Monitoring & Observability**:
- Real-time performance metrics
- Model drift detection
- Alert system vá»›i PagerDuty/Slack
- SLA monitoring (99.9% uptime)

### 2. Scalability

âœ… **High Throughput**:
- 1000+ inference requests per second
- Dynamic batching â†’ 10-100x throughput
- Multi-GPU support
- Horizontal scaling vá»›i Kubernetes

âœ… **Multi-Model Serving**:
- Serve nhiá»u models Ä‘á»“ng thá»i
- Model ensemble
- A/B testing infrastructure
- Resource isolation

### 3. Security & Compliance

âœ… **Data Security**:
- Encryption at rest (model artifacts)
- Encryption in transit (TLS/SSL)
- API authentication (JWT tokens)
- Network isolation (VPC)

âœ… **Compliance**:
- Model explainability logging
- Bias/fairness metrics tracking
- Data lineage documentation
- GDPR compliance (data retention policies)

### 4. Cost Optimization

âœ… **Resource Efficiency**:
- GPU sharing across models
- Auto-scaling based on load
- Spot instance support
- Cost tracking per model

## ğŸ¯ Use Cases trong Banking

### 1. Fraud Detection Model

```python
# Continuously retrain and deploy
# Pipeline: New data â†’ Train â†’ Evaluate â†’ Register â†’ Deploy
# Frequency: Daily
# Monitoring: Precision/Recall, False Positive Rate
```

### 2. Credit Scoring Model

```python
# Quarterly retraining with regulatory approval
# Pipeline: Train â†’ Review â†’ Approve â†’ Stage â†’ Production
# Monitoring: Bias metrics, Performance by segment
```

### 3. Customer Churn Prediction

```python
# Monthly retraining with A/B testing
# Pipeline: Train â†’ Deploy Canary (10%) â†’ Monitor â†’ Full Deploy
# Monitoring: Prediction distribution, Business metrics
```

## ğŸ“š Documentation

- [Architecture Design](docs/architecture.md)
- [MLflow Setup Guide](docs/mlflow-setup.md)
- [Triton Deployment Guide](docs/triton-deployment.md)
- [CI/CD Pipeline](docs/cicd-pipeline.md)
- [Monitoring Setup](docs/monitoring.md)
- [Troubleshooting](docs/troubleshooting.md)

## ğŸ¤ Contributing

```bash
# 1. Fork repository
# 2. Create feature branch
git checkout -b feature/amazing-feature

# 3. Make changes and commit
git commit -m "Add amazing feature"

# 4. Push to branch
git push origin feature/amazing-feature

# 5. Open Pull Request
```

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file

---

<div align="center">

**ğŸ‰ Production-Ready MLOps System! ğŸ‰**

Built with â¤ï¸ for Enterprise ML Teams

[MLflow](https://mlflow.org/) | [Triton](https://github.com/triton-inference-server/server) | [Prometheus](https://prometheus.io/) | [Docker](https://www.docker.com/)

</div>

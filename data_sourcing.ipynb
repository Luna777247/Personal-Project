{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ba9331",
   "metadata": {},
   "source": [
    "# ðŸ“Š Data Sourcing Guide for Projects\n",
    "\n",
    "This notebook demonstrates various methods to source data for your projects, including:\n",
    "1. Installing required libraries\n",
    "2. Downloading from public repositories\n",
    "3. Fetching data via APIs\n",
    "4. Web scraping\n",
    "5. Loading local files\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb2f61",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install requests pandas beautifulsoup4 lxml kaggle-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6586a37",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def download_kaggle_dataset(dataset_name, download_path=\"./data\"):\n",
    "    \"\"\"\n",
    "    Download a dataset from Kaggle\n",
    "    Note: Requires kaggle API key and kaggle package installed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create download directory\n",
    "        Path(download_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Use kaggle API to download\n",
    "        import kaggle\n",
    "        kaggle.api.competition_download_files(dataset_name, path=download_path, quiet=False)\n",
    "\n",
    "        print(f\"Downloaded Kaggle dataset: {dataset_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading from Kaggle: {e}\")\n",
    "        return False\n",
    "\n",
    "def download_from_url(url, filename, download_path=\"./data\"):\n",
    "    \"\"\"\n",
    "    Download file from direct URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Path(download_path).mkdir(parents=True, exist_ok=True)\n",
    "        filepath = os.path.join(download_path, filename)\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "        print(f\"Downloaded file: {filename}\")\n",
    "        return filepath\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# Download from UCI ML Repository\n",
    "# iris_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "# download_from_url(iris_url, \"iris.csv\")\n",
    "\n",
    "# Download from Kaggle (requires API setup)\n",
    "# download_kaggle_dataset(\"titanic\", \"./data/titanic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fafe2",
   "metadata": {},
   "source": [
    "## Download Data from Public Repositories\n",
    "\n",
    "You can download datasets from platforms like Kaggle, UCI Machine Learning Repository, or GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3450cdd",
   "metadata": {},
   "source": [
    "## Fetch Data Using APIs\n",
    "\n",
    "Many services provide APIs to access their data programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a566a",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Fetch GitHub repository information\n",
    "def get_github_repo_info(owner, repo):\n",
    "    \"\"\"\n",
    "    Get information about a GitHub repository\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching GitHub data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# repo_info = get_github_repo_info(\"octocat\", \"Hello-World\")\n",
    "# if repo_info:\n",
    "#     print(f\"Repository: {repo_info['full_name']}\")\n",
    "#     print(f\"Stars: {repo_info['stargazers_count']}\")\n",
    "#     print(f\"Forks: {repo_info['forks_count']}\")\n",
    "\n",
    "# Example: Fetch cryptocurrency data from CoinGecko API\n",
    "def get_crypto_data(crypto_id=\"bitcoin\"):\n",
    "    \"\"\"\n",
    "    Get current cryptocurrency data\n",
    "    \"\"\"\n",
    "    url = f\"https://api.coingecko.com/api/v3/simple/price\"\n",
    "    params = {\n",
    "        'ids': crypto_id,\n",
    "        'vs_currencies': 'usd',\n",
    "        'include_24hr_change': 'true'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching crypto data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# btc_data = get_crypto_data(\"bitcoin\")\n",
    "# if btc_data:\n",
    "#     print(f\"Bitcoin price: ${btc_data['bitcoin']['usd']}\")\n",
    "#     print(f\"24h change: {btc_data['bitcoin']['usd_24h_change']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8774f6a",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Example: Fetch weather data from OpenWeatherMap API\n",
    "def get_weather_data(city, api_key):\n",
    "    \"\"\"\n",
    "    Fetch current weather data for a city\n",
    "    \"\"\"\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    params = {\n",
    "        'q': city,\n",
    "        'appid': api_key,\n",
    "        'units': 'metric'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (replace with your API key)\n",
    "# weather_data = get_weather_data(\"London\", \"your_api_key_here\")\n",
    "# print(json.dumps(weather_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f4733",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_table_from_url(url, table_index=0):\n",
    "    \"\"\"\n",
    "    Scrape a table from a webpage and return as pandas DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        tables = soup.find_all('table')\n",
    "\n",
    "        if table_index >= len(tables):\n",
    "            print(f\"No table found at index {table_index}\")\n",
    "            return None\n",
    "\n",
    "        table = tables[table_index]\n",
    "\n",
    "        # Extract headers\n",
    "        headers = []\n",
    "        header_row = table.find('thead')\n",
    "        if header_row:\n",
    "            headers = [th.text.strip() for th in header_row.find_all('th')]\n",
    "        else:\n",
    "            # Try first row as headers\n",
    "            first_row = table.find('tr')\n",
    "            if first_row:\n",
    "                headers = [th.text.strip() for th in first_row.find_all(['th', 'td'])]\n",
    "\n",
    "        # Extract data rows\n",
    "        rows = []\n",
    "        for row in table.find_all('tr')[1:]:  # Skip header row\n",
    "            cols = [td.text.strip() for td in row.find_all(['td', 'th'])]\n",
    "            if cols:\n",
    "                rows.append(cols)\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# url = \"https://en.wikipedia.org/wiki/List_of_countries_by_population\"\n",
    "# population_data = scrape_table_from_url(url, table_index=0)\n",
    "# print(population_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121e681",
   "metadata": {},
   "source": [
    "## Load Data from Local Files\n",
    "\n",
    "Pandas provides easy methods to load data from various file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f9cf0",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_csv_file(file_path):\n",
    "    \"\"\"Load data from CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded CSV file: {file_path}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load data from JSON file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded JSON file: {file_path}\")\n",
    "        if isinstance(data, list):\n",
    "            print(f\"Number of records: {len(data)}\")\n",
    "        elif isinstance(data, dict):\n",
    "            print(f\"Keys: {list(data.keys())}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_excel_file(file_path, sheet_name=0):\n",
    "    \"\"\"Load data from Excel file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        print(f\"Loaded Excel file: {file_path}\")\n",
    "        print(f\"Sheet: {sheet_name}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Excel: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# csv_data = load_csv_file(\"data/sample.csv\")\n",
    "# json_data = load_json_file(\"data/sample.json\")\n",
    "# excel_data = load_excel_file(\"data/sample.xlsx\")\n",
    "\n",
    "print(\"Data loading functions defined. Ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b24d8c9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated various methods to source data for your projects:\n",
    "\n",
    "1. **Library Installation**: Install necessary packages for data fetching and manipulation\n",
    "2. **Public Repositories**: Download datasets from Kaggle, UCI, or GitHub\n",
    "3. **API Integration**: Fetch real-time data from web services\n",
    "4. **Web Scraping**: Extract data from websites using BeautifulSoup\n",
    "5. **Local Files**: Load data from CSV, JSON, and Excel files\n",
    "\n",
    "### Best Practices:\n",
    "- Always check API terms of service before using\n",
    "- Respect website scraping policies (robots.txt)\n",
    "- Handle errors gracefully in your data fetching code\n",
    "- Validate data quality after loading\n",
    "- Consider data licensing and usage rights\n",
    "\n",
    "### Next Steps:\n",
    "- Choose the appropriate data sourcing method for your project\n",
    "- Implement proper error handling and data validation\n",
    "- Consider data storage and processing pipelines\n",
    "- Document your data sources and collection methods\n",
    "\n",
    "Happy data sourcing! ðŸ“Š"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

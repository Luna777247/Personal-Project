{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "from scripts.inference import DisasterInformationExtractor\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the extractor with trained models\n",
    "models_dir = \"models/\"  # Path to your trained models\n",
    "extractor = DisasterInformationExtractor(models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a02fbd",
   "metadata": {},
   "source": [
    "## Sample Disaster News Articles\n",
    "\n",
    "Let's test the system with some sample Vietnamese disaster news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample disaster news articles\n",
    "sample_articles = [\n",
    "    \"Lũ lụt nghiêm trọng xảy ra tại tỉnh Nghệ An hôm qua, gây thiệt hại hàng trăm tỷ đồng. Chính phủ đã huy động lực lượng cứu hộ khẩn cấp.\",\n",
    "    \n",
    "    \"Động đất mạnh 6.5 độ Richter xảy ra ở huyện Sìn Hồ, Lai Châu vào sáng nay. Không có thương vong nhưng nhiều nhà cửa bị hư hại.\",\n",
    "    \n",
    "    \"Bão số 3 đổ bộ vào các tỉnh miền Trung, gió mạnh lên đến cấp 12. Hàng ngàn hecta lúa và hoa màu bị ngập úng, thiệt hại ước tính 500 tỷ đồng.\",\n",
    "    \n",
    "    \"Hỏa hoạn xảy ra tại khu công nghiệp Long Hậu, Đồng Nai đêm qua. Nguyên nhân ban đầu được xác định do chập điện. Không có thương vong.\",\n",
    "    \n",
    "    \"Sạt lở đất nghiêm trọng tại huyện Mường Tè, Lai Châu do mưa lũ kéo dài. 5 người mất tích, chính quyền địa phương đang tích cực tìm kiếm.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6727e981",
   "metadata": {},
   "source": [
    "## Extract Disaster Information\n",
    "\n",
    "Now let's extract disaster information from each article using our fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each article\n",
    "results = []\n",
    "\n",
    "for i, article in enumerate(sample_articles, 1):\n",
    "    print(f\"\\n=== Article {i} ===\")\n",
    "    print(f\"Text: {article}\")\n",
    "    \n",
    "    # Extract disaster information\n",
    "    disaster_info = extractor.extract_disaster_info(article)\n",
    "    results.append(disaster_info)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nEvent Type: {disaster_info['event_type']} (Confidence: {disaster_info['event_confidence']:.3f})\")\n",
    "    \n",
    "    print(\"\\nExtracted Entities:\")\n",
    "    for entity in disaster_info['entities']:\n",
    "        print(f\"  - {entity['type']}: '{entity['text']}' (confidence: {entity.get('confidence', 'N/A')})\")\n",
    "    \n",
    "    print(\"\\nStructured Information:\")\n",
    "    structured = disaster_info.get('structured_info', {})\n",
    "    for key, value in structured.items():\n",
    "        if value:\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nRelations:\")\n",
    "    for relation in disaster_info['relations']:\n",
    "        head = relation['head']['text']\n",
    "        tail = relation['tail']['text']\n",
    "        rel_type = relation['relation_type']\n",
    "        confidence = relation['confidence']\n",
    "        print(f\"  - {head} --[{rel_type}]--> {tail} (confidence: {confidence:.3f})\")\n",
    "    \n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a25b96",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "For processing multiple articles efficiently, use the batch processing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing\n",
    "print(\"Processing all articles in batch...\")\n",
    "batch_results = extractor.batch_extract(sample_articles)\n",
    "\n",
    "print(f\"Processed {len(batch_results)} articles successfully!\")\n",
    "\n",
    "# Save results to file\n",
    "output_file = \"disaster_extraction_results.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(batch_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a9d7b",
   "metadata": {},
   "source": [
    "## Analysis and Visualization\n",
    "\n",
    "Let's analyze the extraction results and create some visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35ab045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Analyze entity types\n",
    "all_entities = []\n",
    "event_types = []\n",
    "\n",
    "for result in batch_results:\n",
    "    event_types.append(result['event_type'])\n",
    "    for entity in result['entities']:\n",
    "        all_entities.append(entity['type'])\n",
    "\n",
    "# Count frequencies\n",
    "entity_counts = Counter(all_entities)\n",
    "event_counts = Counter(event_types)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Entity types distribution\n",
    "ax1.bar(entity_counts.keys(), entity_counts.values())\n",
    "ax1.set_title('Distribution of Extracted Entity Types')\n",
    "ax1.set_xlabel('Entity Type')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Event types distribution\n",
    "ax2.bar(event_counts.keys(), event_counts.values())\n",
    "ax2.set_title('Distribution of Disaster Event Types')\n",
    "ax2.set_xlabel('Event Type')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('extraction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEntity Type Distribution:\")\n",
    "for entity_type, count in entity_counts.items():\n",
    "    print(f\"  {entity_type}: {count}\")\n",
    "\n",
    "print(\"\\nEvent Type Distribution:\")\n",
    "for event_type, count in event_counts.items():\n",
    "    print(f\"  {event_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fbcdc",
   "metadata": {},
   "source": [
    "## Model Confidence Analysis\n",
    "\n",
    "Let's analyze the confidence scores of our model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence scores\n",
    "event_confidences = [result['event_confidence'] for result in batch_results]\n",
    "relation_confidences = []\n",
    "\n",
    "for result in batch_results:\n",
    "    for relation in result['relations']:\n",
    "        relation_confidences.append(relation['confidence'])\n",
    "\n",
    "# Create confidence analysis plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Event classification confidence\n",
    "ax1.hist(event_confidences, bins=10, alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Event Classification Confidence Distribution')\n",
    "ax1.set_xlabel('Confidence Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Relation extraction confidence\n",
    "if relation_confidences:\n",
    "    ax2.hist(relation_confidences, bins=10, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_title('Relation Extraction Confidence Distribution')\n",
    "    ax2.set_xlabel('Confidence Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No relations extracted', \n",
    "             horizontalalignment='center', verticalalignment='center',\n",
    "             transform=ax2.transAxes, fontsize=12)\n",
    "    ax2.set_title('Relation Extraction Confidence Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confidence_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfidence Statistics:\")\n",
    "print(f\"Event Classification - Mean: {sum(event_confidences)/len(event_confidences):.3f}, Min: {min(event_confidences):.3f}, Max: {max(event_confidences):.3f}\")\n",
    "if relation_confidences:\n",
    "    print(f\"Relation Extraction - Mean: {sum(relation_confidences)/len(relation_confidences):.3f}, Min: {min(relation_confidences):.3f}, Max: {max(relation_confidences):.3f}\")\n",
    "else:\n",
    "    print(\"Relation Extraction - No relations extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12156118",
   "metadata": {},
   "source": [
    "## Export Structured Data\n",
    "\n",
    "Finally, let's export the extracted information in a structured format suitable for further analysis or integration with other systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create structured dataset for further analysis\n",
    "structured_dataset = []\n",
    "\n",
    "for i, result in enumerate(batch_results):\n",
    "    structured_item = {\n",
    "        'article_id': i + 1,\n",
    "        'original_text': result['text'],\n",
    "        'event_type': result['event_type'],\n",
    "        'event_confidence': result['event_confidence'],\n",
    "        'entities': result['entities'],\n",
    "        'relations': result['relations'],\n",
    "        'structured_info': {\n",
    "            'disaster_type': result.get('disaster_type'),\n",
    "            'location': result.get('location', []),\n",
    "            'time': result.get('time'),\n",
    "            'damage': result.get('damage', []),\n",
    "            'response': result.get('response', []),\n",
    "            'impact': result.get('impact', []),\n",
    "            'forecast': result.get('forecast')\n",
    "        }\n",
    "    }\n",
    "    structured_dataset.append(structured_item)\n",
    "\n",
    "# Save structured dataset\n",
    "structured_file = \"structured_disaster_data.json\"\n",
    "with open(structured_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(structured_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Structured dataset saved to {structured_file}\")\n",
    "print(f\"Total articles processed: {len(structured_dataset)}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n=== Processing Summary ===\")\n",
    "print(f\"Total articles: {len(structured_dataset)}\")\n",
    "print(f\"Unique event types: {len(set(item['event_type'] for item in structured_dataset))}\")\n",
    "print(f\"Total entities extracted: {sum(len(item['entities']) for item in structured_dataset)}\")\n",
    "print(f\"Total relations extracted: {sum(len(item['relations']) for item in structured_dataset)}\")\n",
    "\n",
    "# Show sample structured output\n",
    "print(\"\\n=== Sample Structured Output ===\")\n",
    "sample = structured_dataset[0]\n",
    "print(json.dumps({\n",
    "    'article_id': sample['article_id'],\n",
    "    'event_type': sample['event_type'],\n",
    "    'structured_info': sample['structured_info']\n",
    "}, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576d4f1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the complete disaster information extraction pipeline using fine-tuned models optimized for Vietnamese journalism. The system successfully:\n",
    "\n",
    "1. **Entity Recognition**: Extracted disaster-related entities (location, damage, response, etc.)\n",
    "2. **Event Classification**: Classified disaster event types with high confidence\n",
    "3. **Relation Extraction**: Identified relationships between extracted entities\n",
    "4. **Structured Output**: Provided clean, structured data for downstream applications\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Integrate with existing RAG system for enhanced disaster information processing\n",
    "- Deploy models as REST API for real-time processing\n",
    "- Fine-tune on larger, more diverse datasets\n",
    "- Add support for additional entity types and relation types\n",
    "- Implement model monitoring and retraining pipelines\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `disaster_extraction_results.json`: Complete extraction results\n",
    "- `structured_disaster_data.json`: Structured dataset for analysis\n",
    "- `extraction_analysis.png`: Entity and event type distributions\n",
    "- `confidence_analysis.png`: Model confidence distributions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

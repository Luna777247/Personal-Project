import json
import pandas as pd
import re
import glob
from datetime import datetime
import spacy
from transformers import pipeline
import unicodedata
import logging
from tqdm import tqdm
import multiprocessing as mp

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load spaCy model (Vietnamese primary, English fallback)
try:
    nlp_spacy = spacy.load("vi_core_news_lg")  # Vietnamese model as primary
    logger.info("Loaded Vietnamese spaCy model")
except:
    try:
        nlp_spacy = spacy.load("en_core_web_sm")  # English fallback
        logger.warning("Vietnamese spaCy model not found, using English fallback")
    except:
        nlp_spacy = None
        logger.warning("No spaCy model loaded")

# Load Hugging Face NER pipeline for Vietnamese (secondary)
try:
    ner_pipeline = pipeline("ner", model="NlpHUST/ner-vietnamese-electra-base", aggregation_strategy="simple")
    logger.info("Loaded Hugging Face NER model")
except:
    ner_pipeline = None
    logger.warning("Hugging Face NER model not loaded")

# Enhanced regex patterns from extract_data_rule_based.py
weather_patterns = {
    'wind_speed': re.compile(r'(?:suc gio|toc do gio|gio manh|gi√≥ m·∫°nh)\s+(?:manh\s+)?(?:nhat\s+|cuc dai\s+)?(?:cap\s+|c·∫•p\s+)?(\d+)', re.IGNORECASE),
    'gust_speed': re.compile(r'(?:giat cap|giat toc|toc do giat)\s+(\d+)', re.IGNORECASE),
    'pressure': re.compile(r'(?:ap suat|ap luc)\s*(?:thap|cao)?\s*(\d+)', re.IGNORECASE),
    'movement_speed': re.compile(r'(?:toc do|van toc)\s*(?:di chuyen|chuyen dong)?\s*(\d+)-?(\d+)?\s*km/gio', re.IGNORECASE),
    'direction': re.compile(r'(?:huong|phuong)\s+([^,\.\d]+)', re.IGNORECASE),
    'rain_amount': re.compile(r'(?:luong mua|mua|luong mua trung binh)\s+(?:pho bien|trung binh|trong|dat|dat duoc)?\s*(\d+)(?:-(\d+))?\s*mm', re.IGNORECASE),
    'earthquake_magnitude': re.compile(r'(?:do lon|richter|magnitude)\s+(\d+(?:\.\d+)?)', re.IGNORECASE),
    'earthquake_depth': re.compile(r'(?:do sau|chieu sau)\s*(\d+)-?(\d+)?\s*km', re.IGNORECASE),
    'tsunami_height': re.compile(r'(?:do cao|chieu cao song)\s*(\d+)-?(\d+)?\s*m', re.IGNORECASE),
    'fire_area': re.compile(r'(?:dien tich|mat do)\s*(?:chay|bi chay)\s*(\d+(?:\.\d+)?)\s*ha', re.IGNORECASE),
    'drought_duration': re.compile(r'(?:keo dai|thoi gian)\s*(\d+)-?(\d+)?\s*(?:ngay|thang|nam)', re.IGNORECASE),
    'water_shortage': re.compile(r'(?:thieu|han)\s*(\d+(?:\.\d+)?)%?\s*(?:nuoc|nguon nuoc)', re.IGNORECASE),
    'temperature': re.compile(r'(?:nhiet do|do nong)\s*(\d+)-?(\d+)?\s*¬∞?c', re.IGNORECASE),
    'aqi_index': re.compile(r'(?:chi so|AQI|chat luong khong khi)\s*(\d+)', re.IGNORECASE),
    'pollutant_level': re.compile(r'(?:nong do|ham luong)\s*(\d+(?:\.\d+)?)\s*(?:ppm|mg/m¬≥|¬µg/m¬≥)', re.IGNORECASE),
    'volcanic_ash_height': re.compile(r'(?:tro vung|cot tro)\s*(?:cao|chieu cao)\s*(\d+)-?(\d+)?\s*(?:km|m)', re.IGNORECASE),
    'landslide_depth': re.compile(r'(?:do sau|chieu sau)\s*(?:sat lo|truot dat)\s*(\d+)-?(\d+)?\s*m', re.IGNORECASE),
    'oil_spill_area': re.compile(r'(?:dien tich|mat do)\s*(?:tran dau|ro ri dau)\s*(\d+(?:\.\d+)?)\s*(?:km¬≤|hec-ta|m¬≤)', re.IGNORECASE),
    'chemical_concentration': re.compile(r'(?:nong do|ham luong)\s*(?:hoa chat|phong xa)\s*(\d+(?:\.\d+)?)\s*(?:ppm|mg/l|¬µg/l)', re.IGNORECASE),
    'epidemic_cases': re.compile(r'(\d+)\s*(?:ca|nguoi|benh nhan)\s*(?:nhiem|mac|bi)\s*(?:benh|dich)', re.IGNORECASE),
    'invasive_species_area': re.compile(r'(?:dien tich|khu vuc)\s*(?:sinh vat ngoai lai|sinh vat xam hai)\s*(\d+(?:\.\d+)?)\s*(?:ha|km¬≤)', re.IGNORECASE),
    'salinity_level': re.compile(r'(?:do man|ham luong man)\s*(\d+(?:\.\d+)?)\s*(?:g/l|ppt|%)', re.IGNORECASE),
    'frost_temperature': re.compile(r'(?:nhiet do|do lanh)\s*(?:suong muoi|ret dam|ret hai)\s*(-?\d+)-?(-?\d+)?\s*¬∞?c', re.IGNORECASE)
}

damage_patterns = {
    'human_losses': re.compile(r'(\d+(?:[.,]\d+)?)\s*(?:nguoi|ca|ng∆∞·ªùi)\s*(?:chet|thiet mang|tu vong|mat mang|thuong vong|ch·∫øt|t·ª≠ vong|m·∫•t m·∫°ng|th∆∞∆°ng vong)', re.IGNORECASE),
    'injured': re.compile(r'(\d+)\s*(?:nguoi|ca)\s*(?:bi thuong|bi thuong tich|bi thuong nang|bi thuong trong)', re.IGNORECASE),
    'missing': re.compile(r'(\d+)\s*(?:nguoi|ca)\s*(?:mat tich|bi mat tich|thieu)', re.IGNORECASE),
    'economic_loss': re.compile(r'(?:gay|gay ra|ton that|mat mat|thiet hai)(?:\s*(?:kinh te|tai chinh|khoang|du kien|uoc tinh))?\s*[:;]?\s*(\d+(?:[.,]\d+)?)\s*(?:ty|trieu|nghin|t·ª∑|tri·ªáu|ngh√¨n)?\s*(?:dong|usd|\$|ƒë·ªìng)?', re.IGNORECASE),
    'property_damage': re.compile(r'(\d+)\s*(?:ngoi nha|can nha|toa nha|co so ha tang)\s*(?:bi|thiet hai|sap|pha huy)', re.IGNORECASE),
    'evacuated': re.compile(r'(?:so tan|di cu|di dan)\s*(\d+)\s*(?:ho|nguoi|khau|gia dinh)', re.IGNORECASE),
    'houses_destroyed': re.compile(r'(\d+)\s*(?:ngoi nha|can nha|nha)\s*(?:sap|bi sap|bi pha huy|bi cuon troi|bi huy hoai)', re.IGNORECASE),
    'houses_damaged': re.compile(r'(\d+(?:[.,]\d+)?)\s*(?:ngoi nha|can nha|nha|to√† nh√†)\s*(?:bi|thiet hai|sap|pha huy|hu hong|hu hai|h∆∞ h·ªèng|t·ªëc m√°i|ng·∫≠p|anh huong|b·ªã ·∫£nh h∆∞·ªüng)', re.IGNORECASE),
    'roads_damaged': re.compile(r'(\d+(?:\.\d+)?)\s*(?:km|cay so)\s*(?:duong sa|quoc lo|tinh lo|duong)\s*(?:bi hu|bi sat|bi ngap|bi hong)', re.IGNORECASE),
    'bridges_damaged': re.compile(r'(\d+)\s*(?:cay|cau)\s*(?:cau|bac)\s*(?:bi hu|bi sap|bi cuon troi|bi pha huy)', re.IGNORECASE),
    'crops_damaged': re.compile(r'(\d+(?:\.\d+)?)\s*(?:ha|hec-ta)\s*(?:ruong|dong|lua|nong nghiep|dat trong)\s*(?:bi anh huong|bi thiet hai|bi mat)', re.IGNORECASE),
    'livestock_lost': re.compile(r'(\d+)\s*(?:con|vat nuoi|gia suc|gia cam)\s*(?:chet|mat|bi cuon troi|bi huy)', re.IGNORECASE),
    'infrastructure_damage': re.compile(r'(\d+)\s*(?:cong trinh|ha tang|co so)\s*(?:bi hu|bi pha huy|bi hong)', re.IGNORECASE),
    'forest_area_burned': re.compile(r'(\d+(?:\.\d+)?)\s*(?:ha|hec-ta)\s*(?:rung|khu rung)\s*(?:bi chay|chay|bi dot)', re.IGNORECASE),
    'water_shortage_households': re.compile(r'(\d+)\s*(?:ho|gia dinh|nguoi)\s*(?:thieu nuoc|kho han|han han)', re.IGNORECASE),
    'drought_affected_area': re.compile(r'(\d+(?:\.\d+)?)\s*(?:ha|hec-ta)\s*(?:dat|ruong|dien tich)\s*(?:kho han|bi han|han han)', re.IGNORECASE),
    'pollution_affected_people': re.compile(r'(\d+)\s*(?:nguoi|ca)\s*(?:bi anh huong|bi ngo doc|bi tac dong)\s*(?:o nhiem|khong khi)', re.IGNORECASE),
    'health_impact': re.compile(r'(\d+)\s*(?:nguoi|ca)\s*(?:vao vien|cap cuu|bi benh|bi nhiem)\s*(?:do|vi)\s*(?:o nhiem|khong khi)', re.IGNORECASE),
    'volcanic_victims': re.compile(r'(\d+)\s*(?:nguoi|ca)\s*(?:chet|bi hu|bi anh huong)\s*(?:do|vi)\s*(?:nui lua|tro vung)', re.IGNORECASE),
    'landslide_victims': re.compile(r'(\d+)\s*(?:nguoi|ca)\s*(?:chet|mat tich|bi hu)\s*(?:do|vi)\s*(?:sat lo|truot dat)', re.IGNORECASE),
    'epidemic_deaths': re.compile(r'(\d+)\s*(?:nguoi|ca)\s*(?:chet|tu vong)\s*(?:do|vi)\s*(?:dich benh|benh dich)', re.IGNORECASE),
    'epidemic_infected': re.compile(r'(\d+)\s*(?:nguoi|ca)\s*(?:nhiem|mac|bi)\s*(?:dich benh|benh dich)', re.IGNORECASE),
    'animal_epidemic': re.compile(r'(\d+)\s*(?:con|vat nuoi|gia suc)\s*(?:chet|bi huy)\s*(?:do|vi)\s*(?:dich benh)', re.IGNORECASE),
    'crop_epidemic': re.compile(r'(\d+(?:\.\d+)?)\s*(?:ha|hec-ta)\s*(?:ruong|dong|dat trong)\s*(?:bi huy|mat)\s*(?:do|vi)\s*(?:dich benh)', re.IGNORECASE),
    'oil_pollution_area': re.compile(r'(\d+(?:\.\d+)?)\s*(?:km¬≤|hec-ta)\s*(?:bien|hai|song)\s*(?:bi o nhiem|tran dau)', re.IGNORECASE),
    'chemical_accident_victims': re.compile(r'(\d+)\s*(?:nguoi|ca)\s*(?:bi anh huong|nhiem doc|bi hu)\s*(?:do|vi)\s*(?:hoa chat|phong xa)', re.IGNORECASE),
    'marine_life_affected': re.compile(r'(\d+)\s*(?:con|loai)\s*(?:dong vat|ca|tom)\s*(?:bien|hai)\s*(?:chet|bi hu)\s*(?:do|vi)\s*(?:o nhiem)', re.IGNORECASE),
    'salinity_affected_area': re.compile(r'(\d+(?:\.\d+)?)\s*(?:ha|hec-ta)\s*(?:dat|ruong)\s*(?:bi anh huong|han han)\s*(?:do|vi)\s*(?:xam nhap man)', re.IGNORECASE),
    'frost_damage': re.compile(r'(\d+(?:\.\d+)?)\s*(?:ha|hec-ta)\s*(?:ruong|dong|nong nghiep)\s*(?:bi hu|mat)\s*(?:do|vi)\s*(?:suong muoi|ret hai)', re.IGNORECASE)
}

def extract_entities_hf(content):
    """Tr√≠ch xu·∫•t entities b·∫±ng Hugging Face (LOC, MISC, PER, ORG) v·ªõi c·∫£i thi·ªán cleaning"""
    if ner_pipeline:
        # L√†m s·∫°ch text ƒë·ªÉ tr√°nh l·ªói tokenization
        content = unicodedata.normalize('NFKC', content)
        content = re.sub(r'[^\w\s.,!?;:\-()\'\"√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê]', ' ', content)
        
        # C·∫Øt content ƒë·ªÉ tr√°nh l·ªói max length
        content = content[:1000]  # Approximate 512 tokens
        try:
            entities = ner_pipeline(content)
            locs = []
            for ent in entities:
                if ent['entity_group'] == 'LOC':
                    # Lo·∫°i b·ªè token fragments (b·∫Øt ƒë·∫ßu b·∫±ng ##)
                    word = ent['word']
                    if not word.startswith('##'):
                        locs.append(word)
            return list(set(locs))  # Unique locations
        except Exception as e:
            logger.warning(f"NER extraction failed: {e}")
            return []
    return []

def extract_entities_spacy(content):
    """Tr√≠ch xu·∫•t entities b·∫±ng spaCy (GPE, LOC, FAC, ORG) v·ªõi m√¥ h√¨nh ti·∫øng Vi·ªát"""
    if not nlp_spacy:
        return []
    
    try:
        doc = nlp_spacy(content)
        # Extract location-related entities
        locs = []
        for ent in doc.ents:
            if ent.label_ in ['GPE', 'LOC', 'FAC']:  # GPE: geopolitical, LOC: location, FAC: facility
                locs.append(ent.text)
        return list(set(locs))
    except Exception as e:
        logger.warning(f"spaCy extraction failed: {e}")
        return []

def extract_location_rule_based(content):
    """Rule-based extraction cho ƒë·ªãa danh ph·ªï bi·∫øn Vi·ªát Nam"""
    # Danh s√°ch ƒë·ªãa danh m·ªü r·ªông
    vietnam_locations = [
        # T·ªânh th√†nh
        'H√† N·ªôi', 'H·ªì Ch√≠ Minh', 'TP.HCM', 'S√†i G√≤n', 'H·∫£i Ph√≤ng', 'C·∫ßn Th∆°', 'ƒê√† N·∫µng',
        'H√† Giang', 'Cao B·∫±ng', 'B·∫Øc K·∫°n', 'Tuy√™n Quang', 'L√†o Cai', 'ƒêi·ªán Bi√™n', 'Lai Ch√¢u', 'S∆°n La',
        'Y√™n B√°i', 'H√≤a B√¨nh', 'Th√°i Nguy√™n', 'L·∫°ng S∆°n', 'Qu·∫£ng Ninh', 'B·∫Øc Giang', 'Ph√∫ Th·ªç',
        'Vƒ©nh Ph√∫c', 'B·∫Øc Ninh', 'H·∫£i D∆∞∆°ng', 'H∆∞ng Y√™n', 'Th√°i B√¨nh', 'H√† Nam', 'Nam ƒê·ªãnh', 'Ninh B√¨nh',
        'Thanh H√≥a', 'Ngh·ªá An', 'H√† Tƒ©nh', 'Qu·∫£ng B√¨nh', 'Qu·∫£ng Tr·ªã', 'Th·ª´a Thi√™n Hu·∫ø', 'ƒê√† N·∫µng',
        'Qu·∫£ng Nam', 'Qu·∫£ng Ng√£i', 'B√¨nh ƒê·ªãnh', 'Ph√∫ Y√™n', 'Kh√°nh H√≤a', 'Ninh Thu·∫≠n', 'B√¨nh Thu·∫≠n',
        'Kon Tum', 'Gia Lai', 'ƒê·∫Øk L·∫Øk', 'ƒê·∫Øk N√¥ng', 'L√¢m ƒê·ªìng',
        'B√¨nh Ph∆∞·ªõc', 'T√¢y Ninh', 'B√¨nh D∆∞∆°ng', 'ƒê·ªìng Nai', 'B√† R·ªãa V≈©ng T√†u', 'Long An', 'Ti·ªÅn Giang',
        'B·∫øn Tre', 'Tr√† Vinh', 'Vƒ©nh Long', 'ƒê·ªìng Th√°p', 'An Giang', 'Ki√™n Giang', 'H·∫≠u Giang', 'S√≥c TrƒÉng', 'B·∫°c Li√™u', 'C√† Mau',
        # V√πng mi·ªÅn
        'Mi·ªÅn B·∫Øc', 'Mi·ªÅn Trung', 'Mi·ªÅn Nam', 'ƒê·ªìng b·∫±ng s√¥ng H·ªìng', 'ƒê·ªìng b·∫±ng s√¥ng C·ª≠u Long',
        'Trung du mi·ªÅn n√∫i B·∫Øc B·ªô', 'Duy√™n h·∫£i Nam Trung B·ªô', 'T√¢y Nguy√™n', 'ƒê√¥ng Nam B·ªô',
        # Bi·ªÉn ƒë·∫£o
        'Bi·ªÉn ƒê√¥ng', 'Ho√†ng Sa', 'Tr∆∞·ªùng Sa', 'v·ªãnh B·∫Øc B·ªô', 'v·ªãnh H·∫° Long',
        # S√¥ng h·ªì
        's√¥ng H·ªìng', 's√¥ng M√£', 's√¥ng Lam', 's√¥ng C·∫£', 's√¥ng Gianh', 's√¥ng H∆∞∆°ng', 's√¥ng Thu B·ªìn',
        's√¥ng Ba', 's√¥ng ƒê·ªìng Nai', 's√¥ng C·ª≠u Long', 's√¥ng Ti·ªÅn', 's√¥ng H·∫≠u', 'h·ªì Ho√†n Ki·∫øm', 'h·ªì T√¢y'
    ]
    
    found_locations = []
    content_lower = content.lower()
    for loc in vietnam_locations:
        if loc.lower() in content_lower:
            found_locations.append(loc)
    
    return list(set(found_locations))

def extract_location(content):
    """Tr√≠ch xu·∫•t v·ªã tr√≠ ·∫£nh h∆∞·ªüng b·∫±ng NLP k·∫øt h·ª£p (spaCy primary + Hugging Face secondary + rule-based)"""
    # spaCy primary (Vietnamese model)
    locs_spacy = extract_entities_spacy(content)
    
    # Hugging Face secondary
    locs_hf = extract_entities_hf(content)
    
    # Rule-based
    locs_rule = extract_location_rule_based(content)
    
    # Combine all
    all_locs = list(set(locs_spacy + locs_hf + locs_rule))
    
    # Filter to known provinces and regions
    provinces = [
        'Qu·∫£ng Ninh', 'H·∫£i Ph√≤ng', 'H√† N·ªôi', 'Thanh H√≥a', 'Ngh·ªá An', 'H√† Tƒ©nh',
        'Qu·∫£ng B√¨nh', 'Qu·∫£ng Tr·ªã', 'Th·ª´a Thi√™n Hu·∫ø', 'ƒê√† N·∫µng', 'Qu·∫£ng Nam',
        'Qu·∫£ng Ng√£i', 'B√¨nh ƒê·ªãnh', 'Ph√∫ Y√™n', 'Kh√°nh H√≤a', 'Ninh Thu·∫≠n',
        'B√¨nh Thu·∫≠n', 'B√† R·ªãa V≈©ng T√†u', 'T√¢y Ninh', 'ƒê·ªìng Nai', 'Gia Lai',
        'ƒê·∫Øk L·∫Øk', 'Kon Tum', 'L√¢m ƒê·ªìng', 'ƒê·∫Øk N√¥ng', 'Bi·ªÉn ƒê√¥ng', 'Ho√†ng Sa',
        'Tr∆∞·ªùng Sa', 'v·ªãnh B·∫Øc B·ªô', 'Mi·ªÅn B·∫Øc', 'Mi·ªÅn Trung', 'Mi·ªÅn Nam',
        'ƒê·ªìng b·∫±ng s√¥ng C·ª≠u Long', 'ƒê·ªìng b·∫±ng s√¥ng H·ªìng'
    ]
    
    # Additional filtering: remove fragments and short words
    filtered = []
    for loc in all_locs:
        loc_clean = loc.strip()
        if len(loc_clean) >= 2 and not loc_clean.startswith('##') and not re.match(r'^\W+$', loc_clean):
            # Check if it matches or contains known provinces
            if any(prov.lower() in loc_clean.lower() for prov in provinces) or loc_clean in provinces:
                filtered.append(loc_clean)
    
    return ', '.join(filtered) if filtered else None

def extract_quantities_with_ner(content):
    """S·ª≠ d·ª•ng NER ƒë·ªÉ tr√≠ch xu·∫•t quantities (s·ªë li·ªáu)"""
    if not nlp_spacy:
        return {}
    
    try:
        doc = nlp_spacy(content)
        quantities = {}
        for ent in doc.ents:
            if ent.label_ == 'QUANTITY':
                # Parse quantity (e.g., "100 mm", "c·∫•p 6")
                text = ent.text.lower()
                if 'mm' in text:
                    match = re.search(r'(\d+(?:\.\d+)?)', text)
                    if match:
                        quantities['rainfall'] = f"{match.group(1)} mm"
                elif 'km/h' in text or 'kmh' in text:
                    match = re.search(r'(\d+(?:\.\d+)?)', text)
                    if match:
                        quantities['wind_speed'] = f"c·∫•p {min(17, max(1, round(float(match.group(1)) / 5)))}"  # Rough conversion
                elif 'ng∆∞·ªùi' in text and ('ch·∫øt' in text or 'm·∫•t t√≠ch' in text):
                    match = re.search(r'(\d+(?:\.\d+)?)', text)
                    if match:
                        quantities['casualties'] = f"{match.group(1)} ng∆∞·ªùi ch·∫øt"
                elif 't·ª∑' in text and 'ƒë·ªìng' in text:
                    match = re.search(r'(\d+(?:\.\d+)?)', text)
                    if match:
                        quantities['damages'] = f"{match.group(1)} t·ª∑ ƒë·ªìng"
        return quantities
    except Exception as e:
        logger.warning(f"NER quantity extraction failed: {e}")
        return {}

def extract_numbers(content):
    """Tr√≠ch xu·∫•t s·ªë li·ªáu (s·ª©c gi√≥, m∆∞a, thi·ªát h·∫°i) b·∫±ng enhanced regex patterns t·ª´ rule-based extraction"""
    # First, try NER for quantities
    ner_quantities = extract_quantities_with_ner(content)
    
    # Initialize results
    wind_speed = None
    rainfall = None
    casualties = None
    damages = None
    
def extract_numbers(content):
    """Tr√≠ch xu·∫•t s·ªë li·ªáu (s·ª©c gi√≥, m∆∞a, thi·ªát h·∫°i) b·∫±ng enhanced regex patterns t·ª´ rule-based extraction"""
    # First, try NER for quantities
    ner_quantities = extract_quantities_with_ner(content)

    # Initialize results
    wind_speed = None
    rainfall = None
    casualties = None
    damages = None

    # Extract wind speed using enhanced patterns with better coverage
    if not wind_speed:
        # Try multiple wind speed patterns
        wind_patterns = [
            weather_patterns['wind_speed'],
            re.compile(r'(?:gio|gi√≥)\s+(?:manh|toc do)\s+(?:cap|c·∫•p)\s*(\d+)', re.IGNORECASE),
            re.compile(r'(?:suc gio|s·ª©c gi√≥)\s+(?:trung binh|trung b√¨nh|cuc dai|c·ª±c ƒë·∫°i)\s+(?:cap|c·∫•p)\s*(\d+)', re.IGNORECASE),
            re.compile(r'(?:gio|gi√≥)\s+(?:cap|c·∫•p)\s*(\d+)', re.IGNORECASE),
            re.compile(r'(?:toc do gio|t·ªëc ƒë·ªô gi√≥)\s+(\d+)(?:\s*km/h|\s*km/gio)', re.IGNORECASE)
        ]

        for pattern in wind_patterns:
            match = pattern.search(content)
            if match:
                if 'km/h' in content.lower() or 'km/gio' in content.lower():
                    # Convert km/h to wind scale (rough approximation)
                    kmh = int(match.group(1))
                    scale = min(17, max(1, round(kmh / 5)))
                    wind_speed = f"c·∫•p {scale}"
                else:
                    wind_speed = f"c·∫•p {match.group(1)}"
                break

        # Try gust_speed pattern
        match = weather_patterns['gust_speed'].search(content)
        if match and wind_speed:
            wind_speed += f", gi·∫≠t c·∫•p {match.group(1)}"

    # Extract rainfall using enhanced patterns with better coverage
    if not rainfall:
        # Try multiple rainfall patterns
        rain_patterns = [
            weather_patterns['rain_amount'],
            re.compile(r'(?:mua|m∆∞a)\s+(?:trong|dat|pho bien|trung binh)\s+(\d+)(?:-(\d+))?\s*mm', re.IGNORECASE),
            re.compile(r'(?:luong mua|l∆∞·ª£ng m∆∞a)\s+(\d+)(?:-(\d+))?\s*mm', re.IGNORECASE),
            re.compile(r'(?:mua|m∆∞a)\s+(\d+)(?:-(\d+))?\s*mm', re.IGNORECASE),
            re.compile(r'(?:tong luong|t·ªïng l∆∞·ª£ng)\s+(?:mua|m∆∞a)\s+(\d+)(?:-(\d+))?\s*mm', re.IGNORECASE)
        ]

        for pattern in rain_patterns:
            match = pattern.search(content)
            if match:
                amount = match.group(1)
                if match.group(2):  # Range
                    rainfall = f"{amount}-{match.group(2)} mm"
                else:
                    rainfall = f"{amount} mm"
                break

    # Extract casualties using enhanced damage patterns with better coverage
    if not casualties:
        # Try human_losses pattern
        match = damage_patterns['human_losses'].search(content)
        if match:
            casualties = f"{match.group(1)} ng∆∞·ªùi ch·∫øt"

        # Try injured pattern
        match = damage_patterns['injured'].search(content)
        if match and casualties:
            casualties += f", {match.group(1)} ng∆∞·ªùi b·ªã th∆∞∆°ng"
        elif match:
            casualties = f"{match.group(1)} ng∆∞·ªùi b·ªã th∆∞∆°ng"

        # Try missing pattern
        match = damage_patterns['missing'].search(content)
        if match and casualties:
            casualties += f", {match.group(1)} ng∆∞·ªùi m·∫•t t√≠ch"
        elif match:
            casualties = f"{match.group(1)} ng∆∞·ªùi m·∫•t t√≠ch"

        # Additional casualty patterns for better coverage
        casualty_patterns = [
            re.compile(r'(\d+(?:[.,]\d+)?)\s*(?:nguoi|ca|ng∆∞·ªùi)\s*(?:thiet mang|th∆∞∆°ng vong|m·∫•t m·∫°ng)', re.IGNORECASE),
            re.compile(r'(?:gay|gay ra|g√¢y ra)\s*(\d+(?:[.,]\d+)?)\s*(?:nguoi|ca|ng∆∞·ªùi)\s*(?:chet|ch·∫øt)', re.IGNORECASE),
            re.compile(r'(?:so nguoi chet|s·ªë ng∆∞·ªùi ch·∫øt)\s*[:;]?\s*(\d+(?:[.,]\d+)?)', re.IGNORECASE)
        ]

        for pattern in casualty_patterns:
            match = pattern.search(content)
            if match and not casualties:
                casualties = f"{match.group(1)} ng∆∞·ªùi ch·∫øt"
                break

    # Extract damages using enhanced patterns with better coverage
    if not damages:
        # Try economic_loss pattern
        match = damage_patterns['economic_loss'].search(content)
        if match:
            amount = match.group(1)  # The number
            unit_parts = []
            if len(match.groups()) >= 2 and match.group(2):
                unit_parts.append(match.group(2))  # ty|trieu|nghin etc.
            if len(match.groups()) >= 3 and match.group(3):
                unit_parts.append(match.group(3))  # dong|usd|$ etc.
            unit = ' '.join(unit_parts) if unit_parts else "t·ª∑ ƒë·ªìng"
            damages = f"{amount} {unit}"

        # Additional damage patterns
        damage_patterns_extra = [
            re.compile(r'(?:thiet hai|thi·ªát h·∫°i)\s*(?:kinh te|tai chinh)?\s*[:;]?\s*(\d+(?:[.,]\d+)?)\s*(?:ty|trieu|t·ª∑|tri·ªáu)\s*(?:dong|ƒë·ªìng)?', re.IGNORECASE),
            re.compile(r'(?:ton that|t·ªïn th·∫•t)\s*[:;]?\s*(\d+(?:[.,]\d+)?)\s*(?:ty|trieu|t·ª∑|tri·ªáu)', re.IGNORECASE),
            re.compile(r'(?:mat mat|m·∫•t m√°t)\s*[:;]?\s*(\d+(?:[.,]\d+)?)\s*(?:ty|trieu|t·ª∑|tri·ªáu)', re.IGNORECASE)
        ]

        for pattern in damage_patterns_extra:
            match = pattern.search(content)
            if match and not damages:
                damages = f"{match.group(1)} t·ª∑ ƒë·ªìng"
                break

    # Override with NER if available and more specific
    if ner_quantities.get('wind_speed') and not wind_speed:
        wind_speed = ner_quantities['wind_speed']
    if ner_quantities.get('rainfall') and not rainfall:
        rainfall = ner_quantities['rainfall']
    if ner_quantities.get('casualties') and not casualties:
        casualties = ner_quantities['casualties']
    if ner_quantities.get('damages') and not damages:
        damages = ner_quantities['damages']

    return wind_speed, rainfall, casualties, damages

def normalize_damages(damages_str):
    """Chu·∫©n h√≥a damages th√†nh s·ªë (float)"""
    if damages_str:
        match = re.search(r'(\d+(?:\.\d+)?)', damages_str)
        if match:
            return float(match.group(1))
    return None

def extract_severity_level(wind_speed, disaster_type):
    """T√≠nh m·ª©c ƒë·ªô nghi√™m tr·ªçng d·ª±a tr√™n s·ª©c gi√≥ v√† lo·∫°i thi√™n tai"""
    if not wind_speed:
        if disaster_type in ['l≈© l·ª•t', 's·∫°t l·ªü ƒë·∫•t']:
            return 'Trung b√¨nh'
        return 'Kh√¥ng x√°c ƒë·ªãnh'
    
    match = re.search(r'c·∫•p\s*(\d+)', wind_speed)
    if match:
        level = int(match.group(1))
        if level >= 12:
            return 'R·∫•t nghi√™m tr·ªçng'
        elif level >= 10:
            return 'Nghi√™m tr·ªçng'
        elif level >= 8:
            return 'Trung b√¨nh'
        else:
            return 'Nh·∫π'
    return 'Kh√¥ng x√°c ƒë·ªãnh'

def extract_impact_area(content, location):
    """Tr√≠ch xu·∫•t khu v·ª±c ·∫£nh h∆∞·ªüng d·ª±a tr√™n content v√† location"""
    if location:
        return location
    
    # N·∫øu kh√¥ng c√≥ location, t√¨m trong content
    areas = []
    if 'mi·ªÅn B·∫Øc' in content.lower():
        areas.append('Mi·ªÅn B·∫Øc')
    if 'mi·ªÅn Trung' in content.lower():
        areas.append('Mi·ªÅn Trung')
    if 'mi·ªÅn Nam' in content.lower():
        areas.append('Mi·ªÅn Nam')
    if 'ƒë·ªìng b·∫±ng s√¥ng C·ª≠u Long' in content.lower():
        areas.append('ƒê·ªìng b·∫±ng s√¥ng C·ª≠u Long')
    if 'ƒë·ªìng b·∫±ng s√¥ng H·ªìng' in content.lower():
        areas.append('ƒê·ªìng b·∫±ng s√¥ng H·ªìng')
    
    return ', '.join(areas) if areas else 'Kh√¥ng x√°c ƒë·ªãnh'

def extract_forecast(content):
    """Tr√≠ch xu·∫•t ph·∫ßn d·ª± b√°o (ƒë∆°n gi·∫£n: t√¨m c√¢u ch·ª©a 'd·ª± b√°o')"""
    sentences = re.split(r'[.!?]', content)
    for sent in sentences:
        if 'd·ª± b√°o' in sent.lower():
            return sent.strip()
    return None

def extract_event_name(content):
    """Tr√≠ch xu·∫•t t√™n ri√™ng c·ªßa thi√™n tai (nh∆∞ t√™n b√£o, t√™n c∆°n b√£o, ƒë·ªông ƒë·∫•t, s√≥ng th·∫ßn, v.v.)"""
    try:
        # T√¨m t√™n b√£o ri√™ng v·ªõi validation ch·∫∑t ch·∫Ω h∆°n
        storm_patterns = [
            r'mang\s+t√™n\s+([A-Z][a-z]+)',  # mang t√™n Danas (∆∞u ti√™n pattern n√†y)
            r'mang\s+ten\s+([A-Z][a-z]+)',  # mang ten Danas
            r'bao\s+so\s+\d+\s+([^,\.\d]{3,15})',  # b√£o s·ªë 16 Danas
            r'con\s+bao\s+([^,\.\d]{3,15})',  # c∆°n b√£o Danas
            r'(?:bao|b√£o)\s+([A-Z][a-z]+)',  # b√£o/bao Yagi (ch·ªâ b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ hoa)
        ]
        for pattern in storm_patterns:
            storm_match = re.search(pattern, content, re.IGNORECASE)
            if storm_match:
                name = storm_match.group(1).strip()
                # Validation ch·∫∑t ch·∫Ω h∆°n cho t√™n b√£o
                if (len(name) >= 3 and len(name) <= 15 and
                    not re.search(r'\d', name) and  # Kh√¥ng ch·ª©a s·ªë
                    not any(char in name for char in ['(', ')', '[', ']', '{', '}', '|', '\\', '/', '?', '*', '+', '^', '$']) and
                    name.lower() not in ['nhi', 'g·ªìm', 'gi√¥ng', 'quanh', 'la', 'ph·ªß', 'r·ªông', 't·∫°i', '·ªü', 'v√πng', 'khu', 'v·ª±c'] and
                    not any(word in name.lower() for word in ['bao', 'nhieu', 'giong', 'bao', 'quanh', 'la', 'luon', 'hinh', 'anh', 'phu', 'rong', 'nhom', 'tre', 'xe', 'oto', 'dac', 'khu', 'con', 'co', 'taluy', 'duong', 'khoang', 'giua', 'dem', 'hon', 'chua', 'nam', 'nao', 'ghi', 'nhan', 'lu', 'lich', 'su', 'dac', 'biet', 'lon', 'cung', 'xuat', 'hien', 'tai', 'song', 'bac', 'bo', 'nghiem', 'trong', 'vuot', 'muc', 'tren', 'cac', 's√¥ng', 'nh∆∞', 'hien', 'nay', 'tp', 'hcm', 'can', 'canh', 'lich', 'su', 'vua', 'qua'])):
                    return f"B√£o {name}"

        # T√¨m t√™n ƒë·ªông ƒë·∫•t ho·∫∑c tr·∫≠n ƒë·ªông ƒë·∫•t v·ªõi validation ch·∫∑t ch·∫Ω
        earthquake_match = re.search(r'(?:dong dat|ƒë·ªông ƒë·∫•t)\s+([^,\.\d]{3,30})', content, re.IGNORECASE)
        if earthquake_match:
            name = earthquake_match.group(1).strip()
            # Validation ch·∫∑t ch·∫Ω: tr√°nh false matches
            exclude_words = ['·ªü', 't·∫°i', 'khu v·ª±c', 'v√πng', 't·ªânh', 'th√†nh ph·ªë', 'qu·∫≠n', 'huy·ªán', 'x√£', 'th·ªã tr·∫•n',
                           'do', 'v√¨', 'c·ªßa', 'trong', 'ng√†y', 'th√°ng', 'nƒÉm', 'l√∫c', 'khi']
            if (len(name) > 2 and len(name) < 25 and
                not any(word in name.lower() for word in exclude_words) and
                not re.search(r'\d', name) and  # Kh√¥ng ch·ª©a s·ªë
                not any(char in name for char in ['(', ')', '[', ']', '{', '}', '|', '\\', '/', '?', '*', '+', '^', '$']) and  # Kh√¥ng ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát
                name.lower() not in ['m·∫°nh', 'y·∫øu', 'nh·∫π', 'n·∫∑ng', 'l·ªõn', 'nh·ªè', 't·∫°i', '·ªü', 'v√πng', 'khu v·ª±c']):
                return f"ƒê·ªông ƒë·∫•t {name}"

        # T√¨m t√™n s√≥ng th·∫ßn v·ªõi validation
        tsunami_match = re.search(r'(?:song than|s√≥ng th·∫ßn)\s+([^,\.\d]{3,25})', content, re.IGNORECASE)
        if tsunami_match:
            name = tsunami_match.group(1).strip()
            exclude_words = ['·ªü', 't·∫°i', 'khu v·ª±c', 'do', 'v√¨', 'c·ªßa', 'trong', 'ng√†y', 'th√°ng', 'nƒÉm']
            if (len(name) > 2 and len(name) < 20 and
                not any(word in name.lower() for word in exclude_words) and
                not re.search(r'\d', name)):
                return f"S√≥ng th·∫ßn {name}"

        # T√¨m t√™n n√∫i l·ª≠a phun tr√†o v·ªõi validation
        volcano_match = re.search(r'(?:nui lua|n√∫i l·ª≠a)\s+([^,\.\d]{3,25})', content, re.IGNORECASE)
        if volcano_match:
            name = volcano_match.group(1).strip()
            exclude_words = ['·ªü', 't·∫°i', 'khu v·ª±c', 'do', 'v√¨', 'c·ªßa', 'trong']
            if (len(name) > 2 and len(name) < 20 and
                not any(word in name.lower() for word in exclude_words) and
                not re.search(r'\d', name)):
                return f"N√∫i l·ª≠a {name}"

        # T√¨m t√™n ch√°y r·ª´ng v·ªõi patterns m·ªü r·ªông v√† validation ch·∫∑t ch·∫Ω
        fire_patterns = [
            r'(?:chay rung|ch√°y r·ª´ng)\s+([^,\.\d]{3,30})',
            r'(?:vuc chay|v√πng ch√°y|r·ª´ng ch√°y)\s+([^,\.\d]{3,30})',
            r'(?:ch√°y)\s+(?:r·ª´ng|khu v·ª±c)\s+([^,\.\d]{3,30})'
        ]
        for pattern in fire_patterns:
            fire_match = re.search(pattern, content, re.IGNORECASE)
            if fire_match:
                name = fire_match.group(1).strip()
                exclude_words = ['·ªü', 't·∫°i', 'do', 'v√¨', 'c·ªßa', 't·∫°i', 'trong', 'ng√†y', 'th√°ng', 'nƒÉm', 'l√∫c', 'khi']
                if (len(name) > 2 and len(name) < 25 and
                    not any(word in name.lower() for word in exclude_words) and
                    not re.search(r'\d', name) and
                    name.lower() not in ['l·ªõn', 'nh·ªè', 'm·∫°nh', 'y·∫øu', 'nhi·ªÅu', '√≠t']):
                    return f"Ch√°y r·ª´ng {name}"

        # T√¨m t√™n d·ªãch b·ªánh v·ªõi validation
        epidemic_match = re.search(r'(?:dich benh|d·ªãch b·ªánh)\s+([^,\.\d]{3,25})', content, re.IGNORECASE)
        if epidemic_match:
            name = epidemic_match.group(1).strip()
            exclude_words = ['·ªü', 't·∫°i', 'khu v·ª±c', 'do', 'v√¨', 'c·ªßa', 'trong']
            if (len(name) > 2 and len(name) < 20 and
                not any(word in name.lower() for word in exclude_words) and
                not re.search(r'\d', name)):
                return f"D·ªãch b·ªánh {name}"

        # T√¨m t√™n l≈© l·ª•t ho·∫∑c tr·∫≠n l≈© v·ªõi patterns m·ªü r·ªông v√† validation ch·∫∑t ch·∫Ω h∆°n
        flood_patterns = [
            r'(?:tran lu|tr·∫≠n l≈©|l≈© l·ª•t)\s+([^,\.\d]{3,25})',
            r'(?:lu lut|l≈© l·ª•t|l≈©)\s+(?:l·ªõn|to|kh·ªïng l·ªì|kh·ªßng khi·∫øp|v∆∞·ª£t m·ª©c)\s+([^,\.\d]{3,25})',
            r'(?:lu lut|l≈© l·ª•t)\s+([^,\.\d]{3,25})'
        ]
        for pattern in flood_patterns:
            flood_match = re.search(pattern, content, re.IGNORECASE)
            if flood_match:
                name = flood_match.group(1).strip()
                exclude_words = ['·ªü', 't·∫°i', 'khu v·ª±c', 'v√πng', 't·ªânh', 'th√†nh ph·ªë', 'qu·∫≠n', 'huy·ªán', 'x√£', 'th·ªã tr·∫•n',
                               'do', 'v√¨', 'c·ªßa', 'trong', 'ng√†y', 'th√°ng', 'nƒÉm', 'l√∫c', 'khi', 'cho', 'c·ªßa', 'v·ªõi',
                               'nh∆∞', 'hi·ªán', 'nay', 'tp', 'hcm', 'c·∫≠n', 'c·∫£nh', 'l·ªãch', 's·ª≠', 'v·ª´a', 'qua', 'tr√™n',
                               'c√°c', 's√¥ng', 'b·∫Øc', 'b·ªô', 'nghi·ªám', 'trong', 'v∆∞·ª£t', 'm·ª©c', 'ƒë·∫∑c', 'bi·ªát', 'l·ªõn',
                               'c√πng', 'xu·∫•t', 'hi·ªán', 't·∫°i', 's√¥ng', 'ch∆∞a', 'nƒÉm', 'n√†o', 'ghi', 'nh·∫≠n', 'l≈©',
                               'l·ªãch', 's·ª≠', 'ƒë·∫∑c', 'bi·ªát', 'l·ªõn', 'c√πng', 'xu·∫•t', 'hi·ªán', 't·∫°i', 's√¥ng']
                if (len(name) > 2 and len(name) < 20 and
                    not any(word in name.lower() for word in exclude_words) and
                    not re.search(r'\d', name) and
                    name.lower() not in ['l·ªõn', 'nh·ªè', 'm·∫°nh', 'y·∫øu', 'nhi·ªÅu', '√≠t', 't·∫°i', '·ªü', 'v√πng', 'khu v·ª±c', 'ƒë·∫•t', 'bi·ªÉn', 's√¥ng', 'su·ªëi'] and
                    not any(char in name for char in ['(', ')', '[', ']', '{', '}', '|', '\\', '/', '?', '*', '+', '^', '$'])):
                    return f"L≈© l·ª•t {name}"

        # T√¨m t√™n h·∫°n h√°n v·ªõi validation
        drought_match = re.search(r'(?:han han|h·∫°n h√°n)\s+([^,\.\d]{3,25})', content, re.IGNORECASE)
        if drought_match:
            name = drought_match.group(1).strip()
            exclude_words = ['·ªü', 't·∫°i', 'khu v·ª±c', 'do', 'v√¨', 'c·ªßa', 'trong']
            if (len(name) > 2 and len(name) < 20 and
                not any(word in name.lower() for word in exclude_words) and
                not re.search(r'\d', name)):
                return f"H·∫°n h√°n {name}"

        # T√¨m t√™n s·∫°t l·ªü/tr∆∞·ª£t ƒë·∫•t v·ªõi validation ch·∫∑t ch·∫Ω h∆°n
        landslide_match = re.search(r'(?:sat lo|tr∆∞·ª£t ƒë·∫•t|s·∫°t l·ªü)\s+([^,\.\d]{3,20})', content, re.IGNORECASE)
        if landslide_match:
            name = landslide_match.group(1).strip()
            exclude_words = ['·ªü', 't·∫°i', 'khu v·ª±c', 'do', 'v√¨', 'c·ªßa', 'trong', 'ng√†y', 'th√°ng', 'nƒÉm', 'l√∫c', 'khi',
                           'ƒë·∫•t', 'b·ªù', 'su·ªëi', 's√¥ng', 'bi·ªÉn', 'n√∫i', 'ƒë·ªìi', 'd·ªëc', 's∆∞·ªùn', 'taluy', 'd∆∞∆°ng', 'kho·∫£ng',
                           'gi·ªØa', 'ƒë√™m', 'h∆°n', 'ch∆∞a', 'nƒÉm', 'n√†o', 'ghi', 'nh·∫≠n', 'nghi·ªám', 'tr·ªçng', 'b·ªù', 's√¥ng',
                           'b·ªì', 'di·ªÖn', 'bi·∫øn', 'ph·ª©c', 't·∫°p', 'm√πa', 'b√£o', 'l≈©']
            if (len(name) > 2 and len(name) < 15 and
                not any(word in name.lower() for word in exclude_words) and
                not re.search(r'\d', name) and
                name.lower() not in ['l·ªõn', 'nh·ªè', 'm·∫°nh', 'y·∫øu', 'nhi·ªÅu', '√≠t', 't·∫°i', '·ªü', 'v√πng', 'khu v·ª±c'] and
                not any(char in name for char in ['(', ')', '[', ']', '{', '}', '|', '\\', '/', '?', '*', '+', '^', '$'])):
                return f"S·∫°t l·ªü {name}"

        return None
    except Exception as e:
        logger.warning(f"Error extracting event name: {e}")
        return None

def filter_relevant_articles(df):
    """L·ªçc b·ªè c√°c b√†i vi·∫øt kh√¥ng li√™n quan ƒë·∫øn thi√™n tai th·ª±c s·ª± v√† qu√° c≈©"""
    # Keywords indicating relevant disaster articles
    disaster_keywords = [
        'b√£o', '√°p th·∫•p nhi·ªát ƒë·ªõi', 'l≈©', 'l≈© qu√©t', 'ng·∫≠p √∫ng', 'h·∫°n h√°n', 'x√¢m nh·∫≠p m·∫∑n',
        'ƒë·ªông ƒë·∫•t', 's√≥ng th·∫ßn', 'n√∫i l·ª≠a', 's·∫°t l·ªü', 'tr∆∞·ª£t ƒë·∫•t', 'ch√°y r·ª´ng', '√¥ nhi·ªÖm',
        'tr√†n d·∫ßu', 's·ª± c·ªë h√≥a ch·∫•t', 'd·ªãch b·ªánh', 'sinh v·∫≠t ngo·∫°i lai', 's∆∞∆°ng mu·ªëi',
        'r√©t ƒë·∫≠m', 'r√©t h·∫°i', 'n·∫Øng n√≥ng', 's√≥ng nhi·ªát', 'm∆∞a l·ªõn', 'l·ªëc xo√°y', 'v√≤i r·ªìng'
    ]
    
    def is_relevant(row):
        title = str(row.get('title', '')).lower()
        content = str(row.get('content', '')).lower()
        disaster_type = str(row.get('disaster_type', '')).lower()
        
        # Check if any keyword appears in title, content, or disaster_type
        text_to_check = title + ' ' + content + ' ' + disaster_type
        return any(keyword in text_to_check for keyword in disaster_keywords)
    
    initial_count = len(df)
    df_filtered = df[df.apply(is_relevant, axis=1)]
    
    # Filter by date: keep articles from 2020 onwards
    if 'date' in df_filtered.columns:
        df_filtered['date'] = pd.to_datetime(df_filtered['date'], errors='coerce')
        df_filtered = df_filtered[df_filtered['date'].dt.year >= 2020]
    
    filtered_count = len(df_filtered)
    logger.info(f"Filtered out {initial_count - filtered_count} irrelevant/old articles")
    
    return df_filtered

def clean_duplicates(df):
    """Lo·∫°i b·ªè duplicate d·ª±a tr√™n url v√† n·ªôi dung t∆∞∆°ng t·ª±"""
    # First, remove exact URL duplicates
    df = df.drop_duplicates(subset=['url'], keep='first')
    
    # Then, remove near-duplicates based on title similarity (optional, can be expensive)
    # For now, just keep URL deduplication as it's fast and effective
    
    return df

def main():
    logger.info("Starting enhanced CSV export process")
    
    # T√¨m file JSON m·ªõi nh·∫•t trong th∆∞ m·ª•c data
    json_files = glob.glob('data/disaster_data_multisource_*.json')
    if not json_files:
        logger.error("‚ùå Kh√¥ng t√¨m th·∫•y file JSON disaster_data_multisource!")
        return
    
    json_file = max(json_files)  # L·∫•y file m·ªõi nh·∫•t
    logger.info(f"üìÇ ƒêang x·ª≠ l√Ω file: {json_file}")
    
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # T·∫°o DataFrame t·ª´ JSON
    df = pd.DataFrame(data)
    logger.info(f"Loaded {len(df)} articles")

    # L·ªçc b·ªè b√†i vi·∫øt kh√¥ng li√™n quan
    df = filter_relevant_articles(df)

    # L√†m s·∫°ch: Lo·∫°i duplicate d·ª±a tr√™n url
    df = clean_duplicates(df)
    logger.info(f"After deduplication: {len(df)} articles")

    # C·∫≠p nh·∫≠t c·ªôt v·ªõi NLP using batch processing for better performance
    logger.info("Extracting locations...")
    df['location'] = [extract_location(content) for content in tqdm(df['content'], desc="Location extraction")]

    logger.info("Extracting numerical data...")
    numerical_data = [extract_numbers(content) for content in tqdm(df['content'], desc="Numerical extraction")]
    df[['wind_speed', 'rainfall', 'casualties', 'damages']] = pd.DataFrame(numerical_data, index=df.index)

    df['damages_normalized'] = df['damages'].apply(normalize_damages)
    df['forecast'] = df['content'].apply(extract_forecast)
    df['event_name'] = df['content'].apply(extract_event_name)

    # Th√™m c·ªôt m·ªõi
    df['severity_level'] = df.apply(lambda row: extract_severity_level(row['wind_speed'], row['disaster_type']), axis=1)
    df['impact_area'] = df.apply(lambda row: extract_impact_area(row['content'], row['location']), axis=1)

    # S·∫Øp x·∫øp c·ªôt (lo·∫°i b·ªè content ƒë·ªÉ nh·∫π file)
    columns = [
        'date', 'disaster_type', 'event_name', 'location', 'impact_area', 'severity_level', 'title', 'source', 'category',
        'wind_speed', 'rainfall', 'casualties', 'damages', 'damages_normalized', 'forecast',
        'url', 'scrape_time'
    ]
    df = df[columns]

    # Xu·∫•t CSV m·ªõi
    new_csv_file = 'data/disaster_data_enhanced.csv'
    df.to_csv(new_csv_file, index=False, encoding='utf-8-sig')
    logger.info(f"ƒê√£ l√†m s·∫°ch v√† xu·∫•t CSV m·ªõi: {new_csv_file}")
    logger.info(f"S·ªë d√≤ng sau l√†m s·∫°ch: {len(df)}")

    # Hi·ªÉn th·ªã th·ªëng k√™
    logger.info("Generating statistics...")
    print(f"\nData Quality Summary:")
    for col in ['event_name', 'location', 'wind_speed', 'rainfall', 'casualties', 'damages']:
        if col in df.columns:
            filled = df[col].notna().sum()
            rate = (filled / len(df)) * 100
            print(f"  {col}: {filled}/{len(df)} ({rate:.1f}%)")
    
    # Validation: Show sample of extracted data for manual check
    print(f"\nSample Validation (first 5 rows):")
    sample_cols = ['title', 'event_name', 'location', 'wind_speed', 'rainfall', 'casualties', 'damages']
    print(df[sample_cols].head().to_string(index=False))
    
    # Cross-reference check: Compare with known sources (placeholder)
    logger.info("Validation complete - manual review recommended for accuracy")

if __name__ == "__main__":
    try:
        main()
        logger.info("Process completed successfully")
    except Exception as e:
        logger.error(f"Process failed: {e}", exc_info=True)
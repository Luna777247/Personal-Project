import requests
from bs4 import BeautifulSoup
import trafilatura
import time
import re
import json
import pandas as pd
from datetime import datetime
from urllib.parse import quote, urljoin
from collections import defaultdict
import urllib3
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util.retry import Retry
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import glob
from transformers import pipeline
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Set up data directory path
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(SCRIPT_DIR, '..', 'data')
os.makedirs(DATA_DIR, exist_ok=True)  # Ensure data directory exists

# T·∫Øt warning SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Danh s√°ch c√°c lo·∫°i thi√™n tai
DISASTER_TYPES = [
    ("Thi√™n tai kh√≠ t∆∞·ª£ng ‚Äì th·ªßy vƒÉn", "B√£o, √°p th·∫•p nhi·ªát ƒë·ªõi"),
    ("Thi√™n tai kh√≠ t∆∞·ª£ng ‚Äì th·ªßy vƒÉn", "L·ªëc xo√°y, v√≤i r·ªìng"),
    ("Thi√™n tai kh√≠ t∆∞·ª£ng ‚Äì th·ªßy vƒÉn", "M∆∞a l·ªõn k√©o d√†i"),
    ("Thi√™n tai kh√≠ t∆∞·ª£ng ‚Äì th·ªßy vƒÉn", "L≈©, l≈© qu√©t"),
    ("Thi√™n tai kh√≠ t∆∞·ª£ng ‚Äì th·ªßy vƒÉn", "Ng·∫≠p √∫ng"),
    ("Thi√™n tai kh√≠ t∆∞·ª£ng ‚Äì th·ªßy vƒÉn", "H·∫°n h√°n"),
    ("Thi√™n tai kh√≠ t∆∞·ª£ng ‚Äì th·ªßy vƒÉn", "X√¢m nh·∫≠p m·∫∑n"),
    ("Thi√™n tai kh√≠ t∆∞·ª£ng ‚Äì th·ªßy vƒÉn", "S∆∞∆°ng mu·ªëi, r√©t ƒë·∫≠m ‚Äì r√©t h·∫°i"),
    ("Thi√™n tai kh√≠ t∆∞·ª£ng ‚Äì th·ªßy vƒÉn", "N·∫Øng n√≥ng, s√≥ng nhi·ªát"),
    ("Thi√™n tai ƒë·ªãa ch·∫•t", "ƒê·ªông ƒë·∫•t"),
    ("Thi√™n tai ƒë·ªãa ch·∫•t", "S√≥ng th·∫ßn"),
    ("Thi√™n tai ƒë·ªãa ch·∫•t", "N√∫i l·ª≠a phun"),
    ("Thi√™n tai ƒë·ªãa ch·∫•t", "S·∫°t l·ªü ƒë·∫•t, tr∆∞·ª£t ƒë·∫•t, s·ª•t l√∫n"),
    ("Thi√™n tai ƒë·ªãa ch·∫•t", "Hang ƒë·ªông karst s·ª•p ƒë·ªï"),
    ("Thi√™n tai sinh h·ªçc", "D·ªãch b·ªánh ·ªü ng∆∞·ªùi"),
    ("Thi√™n tai sinh h·ªçc", "D·ªãch b·ªánh ·ªü ƒë·ªông v·∫≠t"),
    ("Thi√™n tai sinh h·ªçc", "D·ªãch b·ªánh c√¢y tr·ªìng"),
    ("Thi√™n tai sinh h·ªçc", "Sinh v·∫≠t ngo·∫°i lai x√¢m h·∫°i"),
    ("Thi√™n tai m√¥i tr∆∞·ªùng ‚Äì con ng∆∞·ªùi g√¢y ra", "Ch√°y r·ª´ng"),
    ("Thi√™n tai m√¥i tr∆∞·ªùng ‚Äì con ng∆∞·ªùi g√¢y ra", "√î nhi·ªÖm m√¥i tr∆∞·ªùng nghi√™m tr·ªçng"),
    ("Thi√™n tai m√¥i tr∆∞·ªùng ‚Äì con ng∆∞·ªùi g√¢y ra", "Tr√†n d·∫ßu"),
    ("Thi√™n tai m√¥i tr∆∞·ªùng ‚Äì con ng∆∞·ªùi g√¢y ra", "S·ª± c·ªë h√≥a ch·∫•t, ph√≥ng x·∫°")
]

# C·∫•u h√¨nh cho t·ª´ng trang b√°o (ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t)
NEWS_SOURCES = {
    'vnexpress': {
        'name': 'VnExpress',
        'search_url': 'https://timkiem.vnexpress.net/?q={query}&media_type=text&search_f=title,tag_list&page={page}',
        'article_selector': 'article.item-news a[data-medium="Item-1"], article.item-news h3.title-news a, h3.title-news a[title]',
        'pagination_type': 'url',
        'max_pages': 5,
        'needs_ssl_workaround': False,
        'base_domain': 'vnexpress.net'
    },
    'baotintuc': {
        'name': 'B√°o Tin T·ª©c',
        'search_url': 'https://baotintuc.vn/tim-kiem.htm?q={query}&p={page}',
        'article_selector': '.story h2 a, .story__heading a, .item-news h3 a, h3 a.title',
        'pagination_type': 'url',
        'max_pages': 5,
        'needs_ssl_workaround': True,
        'base_domain': 'baotintuc.vn'
    },
    'sggp': {
        'name': 'S√†i G√≤n Gi·∫£i Ph√≥ng',
        'search_url': 'https://www.sggp.org.vn/tim-kiem/?q={query}&page={page}',
        'article_selector': 'h3.article-title a, h2.article-title a, .story-item h3 a',
        'pagination_type': 'url',
        'max_pages': 5,
        'needs_ssl_workaround': False
    },
    'vietnamnet': {
        'name': 'VietnamNet',
        'search_url': 'https://vietnamnet.vn/tim-kiem-p{page}?q={query}',
        'article_selector': 'h3.vnn-title a, .vnn-search-item h3 a, .horizontalPost__main-title a, .article-title a',
        'pagination_type': 'url',
        'max_pages': 5,
        'needs_ssl_workaround': False
    },
    'dantri': {
        'name': 'D√¢n Tr√≠',
        'search_url': 'https://dantri.com.vn/tim-kiem/{query}.htm?pi={page}',
        'article_selector': 'h3.article-title a, h4.article-title a, article h2 a',
        'pagination_type': 'url',
        'max_pages': 5,
        'needs_ssl_workaround': False
    },
    'thanhnien': {
        'name': 'Thanh Ni√™n',
        'search_url': 'https://thanhnien.vn/tim-kiem?keywords={query}',
        'article_selector': 'h2.story__heading a, h3.story__heading a, .story h3 a',
        'pagination_type': 'scroll',
        'max_pages': 3,
        'needs_ssl_workaround': False
    },
    'tuoitre': {
        'name': 'Tu·ªïi Tr·∫ª',
        'search_url': 'https://tuoitre.vn/tim-kiem.htm?keywords={query}',
        'article_selector': 'h3.title-news a, a.box-category-link-title, .story h3 a',
        'pagination_type': 'scroll',
        'max_pages': 3,
        'needs_ssl_workaround': False
    },
    'nld': {
        'name': 'Ng∆∞·ªùi Lao ƒê·ªông',
        'search_url': 'https://nld.com.vn/tim-kiem.htm?keywords={query}&trang={page}',
        'article_selector': 'h3.art-title a, article a.title-news, .story h2 a, .item-news h3 a',
        'pagination_type': 'url',
        'max_pages': 5,
        'needs_ssl_workaround': False
    },
    'qdnd': {
        'name': 'Qu√¢n ƒê·ªôi Nh√¢n D√¢n',
        'search_url': 'https://www.qdnd.vn/tim-kiem/q/{query}/p/{page}',
        'article_selector': 'h3.cms-title a, h2.cms-title a, .article-item h3 a',
        'pagination_type': 'url',
        'max_pages': 5,
        'needs_ssl_workaround': False
    }
}

# Custom SSL Adapter ƒë·ªÉ x·ª≠ l√Ω c√°c trang b√°o d√πng SSL c≈©
class SSLAdapter(HTTPAdapter):
    def init_poolmanager(self, *args, **kwargs):
        ctx = ssl.SSLContext(ssl.PROTOCOL_TLS)
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        # Cho ph√©p c√°c cipher c≈© h∆°n
        ctx.set_ciphers('DEFAULT@SECLEVEL=1')
        # Cho ph√©p unsafe legacy renegotiation (0x00040000)
        ctx.options |= 0x00040000
        kwargs['ssl_context'] = ctx
        return super().init_poolmanager(*args, **kwargs)

class NewsScraperMultiSource:
    def __init__(self):
        self.session = requests.Session()

        # C·∫•u h√¨nh retry strategy
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )

        # Mount adapter v·ªõi retry
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)

        # Mount SSL adapter cho c√°c trang c·∫ßn workaround
        ssl_adapter = SSLAdapter()
        self.session.mount("https://", ssl_adapter)

        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })

        self.results = []

    def extract_urls_from_source(self, source_key, query, max_pages=5):
        """Thu th·∫≠p URLs t·ª´ m·ªôt ngu·ªìn b√°o c·ª• th·ªÉ"""
        source = NEWS_SOURCES[source_key]
        all_urls = set()

        print(f"\n{'='*80}")
        print(f"Ngu·ªìn: {source['name']} | Truy v·∫•n: {query}")
        print(f"{'='*80}")

        if source['pagination_type'] == 'url':
            for page in range(1, min(max_pages + 1, source['max_pages'] + 1)):
                urls = self._extract_from_page(source_key, query, page)
                if not urls:
                    print(f"  Kh√¥ng t√¨m th·∫•y b√†i ·ªü trang {page}. D·ª´ng.")
                    break

                new_urls = urls - all_urls
                if new_urls:
                    all_urls.update(new_urls)
                    print(f"  Trang {page}: Th√™m {len(new_urls)} URL m·ªõi")
                else:
                    print(f"  Trang {page}: Kh√¥ng c√≥ URL m·ªõi, d·ª´ng ph√¢n trang.")
                    break

                time.sleep(1)
        else:
            # Trang d√πng infinite scroll - ch·ªâ l·∫•y trang ƒë·∫ßu
            urls = self._extract_from_page(source_key, query, 1)
            all_urls.update(urls)

        print(f"  ‚Üí T·ªïng URL t·ª´ {source['name']}: {len(all_urls)}")
        return list(all_urls)

    def _extract_from_page(self, source_key, query, page):
        """Tr√≠ch xu·∫•t URLs t·ª´ m·ªôt trang c·ª• th·ªÉ"""
        source = NEWS_SOURCES[source_key]
        urls = set()

        try:
            # Format query cho t·ª´ng trang
            if source_key == 'dantri':
                formatted_query = query.replace(' ', '+').replace(',', '%2c')
            else:
                formatted_query = quote(query)

            # T·∫°o URL t√¨m ki·∫øm
            if '{page}' in source['search_url']:
                search_url = source['search_url'].format(query=formatted_query, page=page)
            else:
                search_url = source['search_url'].format(query=formatted_query)

            print(f"  Trang {page}: {search_url[:100]}...")

            # X√°c ƒë·ªãnh c√≥ c·∫ßn s·ª≠ d·ª•ng SSL workaround kh√¥ng
            verify_ssl = False  # S·ª≠ d·ª•ng SSL adapter t√πy ch·ªânh cho t·∫•t c·∫£

            response = self.session.get(
                search_url,
                timeout=15,
                verify=verify_ssl,
                allow_redirects=True
            )

            if response.status_code != 200:
                print(f"    HTTP {response.status_code}: {response.reason}")
                return []

            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')

            # T√¨m c√°c th·∫ª a theo selector
            articles = soup.select(source['article_selector'])

            # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ c√°c selector chung
            if not articles:
                print(f"    Kh√¥ng t√¨m th·∫•y v·ªõi selector ch√≠nh, th·ª≠ selector d·ª± ph√≤ng...")

                # Th·ª≠ t√¨m theo class ch·ª©a keyword
                possible_selectors = [
                    'a[href*="/"]',  # T·∫•t c·∫£ link
                    'h2 a', 'h3 a', 'h4 a',  # Ti√™u ƒë·ªÅ
                    '.title a', '.news-title a', '.article-title a',  # Class title
                    'article a', '.news-item a', '.story a',  # Container
                ]

                for selector in possible_selectors:
                    articles = soup.select(selector)
                    if articles:
                        print(f"    T√¨m th·∫•y {len(articles)} elements v·ªõi selector: {selector}")
                        break

            # L·ªçc v√† x·ª≠ l√Ω URLs
            base_domain = source.get('base_domain', source['search_url'].split('/')[2])

            for article in articles:
                href = article.get('href', '')
                if not href or href == '#':
                    continue

                # X·ª≠ l√Ω URL t∆∞∆°ng ƒë·ªëi
                if href.startswith('/'):
                    protocol = 'https' if 'https://' in source['search_url'] else 'http'
                    href = f"{protocol}://{base_domain}{href}"
                elif href.startswith('//'):
                    href = f"https:{href}"
                elif not href.startswith('http'):
                    continue

                # Skip patterns chung
                skip_patterns = [
                    'video', 'gallery', 'photo', 'javascript:', 'mailto:',
                    '/tag/', '/topic/', '/tags', '-tags', '.tag',
                    '/tin-tuc-24h/', '/category', '/rss', '.rss',
                    'facebook.com', 'twitter.com', 'zalo.me', 'youtube.com',
                    '/lien-he', '/dieu-khoan', '/chinh-sach', '/gioi-thieu',
                    '/static/', '/nguoi-lao-dong-news.htm',
                    '/moitruongdothi/', '/tin-nong-trong-ngay/',
                    '/tieu-dung-thong-minh', '/giai-tri.htm',
                    '/lien-he.htm', '/contact', '/about',
                    '/chuyen-muc/', '/category/', '/chu-de/',
                ]

                # Skip patterns ƒë·∫∑c th√π cho t·ª´ng trang
                source_specific_skips = {
                    'nld': ['/suc-khoe/', '/giao-duc/', '/van-hoa/', '/the-thao/'],
                    'vietnamnet': ['/ban-doc/', '/giao-duc/', '/suc-khoe/', '/doi-song/', '/giai-tri/'],
                    'thanhnien': ['/lien-he', '/giai-tri.htm'],
                    'tuoitre': ['/lien-he', '/hoi-dap/'],
                }

                if source_key in source_specific_skips:
                    skip_patterns.extend(source_specific_skips[source_key])

                if any(pattern in href.lower() for pattern in skip_patterns):
                    continue

                # Ki·ªÉm tra URL c√≥ pattern b√†i vi·∫øt
                has_article_pattern = any([
                    re.search(r'-\d{6,}', href),  # C√≥ s·ªë d√†i (ID b√†i vi·∫øt)
                    re.search(r'\d{4}-\d{2}-\d{2}', href),  # C√≥ ng√†y th√°ng
                    re.search(r'/\d{4}/', href),  # C√≥ nƒÉm trong ƒë∆∞·ªùng d·∫´n
                    '/thoi-su/' in href, '/xa-hoi/' in href, '/kinh-te/' in href,
                    '.html' in href, '.htm' in href,
                    re.search(r'/\d+\.html$', href),  # K·∫øt th√∫c b·∫±ng s·ªë.html
                ])

                # Ch·ªâ l·∫•y URL thu·ªôc domain c·ªßa ngu·ªìn
                if base_domain in href and len(href) > 30:
                    if has_article_pattern or self._looks_like_article_url(href, source_key):
                        urls.add(href)

            print(f"    T√¨m th·∫•y: {len(urls)} URLs ti·ªÅm nƒÉng")

        except Exception as e:
            print(f"    L·ªói khi tr√≠ch xu·∫•t trang {page}: {e}")

        return urls

    def _looks_like_article_url(self, url, source_key):
        """Ki·ªÉm tra heuristic xem URL c√≥ gi·ªëng b√†i vi·∫øt kh√¥ng"""
        # C√°c pattern ƒë·∫∑c th√π cho t·ª´ng trang
        patterns_by_source = {
            'vnexpress': [r'vnexpress\.net/[^/]+-\d+\.html'],
            'dantri': [r'dantri\.com\.vn/[^/]+/\d+\.htm'],
            'thanhnien': [r'thanhnien\.vn/[^/]+-\d+\.html'],
            'tuoitre': [r'tuoitre\.vn/[^/]+-\d+\.htm'],
            'nld': [r'nld\.com\.vn/[^/]+-\d+\.html'],
            'vietnamnet': [r'vietnamnet\.vn/[^/]+-\d+\.html'],
            'qdnd': [r'qdnd\.vn/[^/]+-\d+\.html'],
            'sggp': [r'sggp\.org\.vn/[^/]+-\d+\.html'],
        }

        if source_key in patterns_by_source:
            for pattern in patterns_by_source[source_key]:
                if re.search(pattern, url):
                    return True

        return False

    def extract_content_and_date(self, url, source_name, disaster_type=None):
        """Tr√≠ch xu·∫•t n·ªôi dung v√† metadata t·ª´ URL"""
        max_retries = 2
        for attempt in range(max_retries):
            try:
                # X√°c ƒë·ªãnh c√≥ c·∫ßn verify SSL kh√¥ng
                verify_ssl = True
                for source_key, config in NEWS_SOURCES.items():
                    if config['name'] == source_name and config.get('needs_ssl_workaround', False):
                        verify_ssl = False
                        break

                response = self.session.get(url, timeout=20, verify=verify_ssl)
                response.raise_for_status()

                # D√πng Trafilatura
                extracted_data = trafilatura.extract(
                    response.text,
                    output_format='json',
                    with_metadata=True,
                    include_comments=False,
                    include_tables=False,
                    no_fallback=False
                )

                content = "Kh√¥ng c√≥ n·ªôi dung"
                date = "N/A"
                title = "N/A"

                if extracted_data:
                    try:
                        data_dict = json.loads(extracted_data)
                        content = data_dict.get('text', "Kh√¥ng c√≥ n·ªôi dung")
                        title = data_dict.get('title', "N/A")
                        date_from_traf = data_dict.get('date')
                        if date_from_traf:
                            # X·ª≠ l√Ω nhi·ªÅu ƒë·ªãnh d·∫°ng date
                            date_match = re.search(r'\d{4}-\d{2}-\d{2}', date_from_traf)
                            if date_match:
                                date = date_match.group(0)
                    except:
                        content = "Kh√¥ng c√≥ n·ªôi dung"

                # Fallback v·ªõi BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')

                # Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ
                if title == "N/A":
                    # Th·ª≠ nhi·ªÅu c√°ch l·∫•y title
                    title_selectors = [
                        'h1', 'h1.title', 'h1.article-title',
                        'h1.title-detail', 'h1.title-news',
                        'meta[property="og:title"]',
                        'meta[name="twitter:title"]',
                        'title'
                    ]

                    for selector in title_selectors:
                        element = soup.select_one(selector)
                        if element:
                            if selector.startswith('meta'):
                                title = element.get('content', 'N/A')
                            else:
                                title = element.get_text(strip=True)
                            if title and title != "N/A":
                                break

                # Validate title d·ª±a tr√™n t·ª´ kh√≥a thi√™n tai
                if disaster_type and title != "N/A":
                    disaster_keywords = disaster_type.lower().split(', ')
                    title_lower = title.lower()
                    has_keyword = any(keyword in title_lower for keyword in disaster_keywords)
                    if not has_keyword:
                        # Ki·ªÉm tra trong content n·∫øu c√≥
                        content_lower = content.lower() if content != "Kh√¥ng c√≥ n·ªôi dung" else ""
                        has_keyword = any(keyword in content_lower for keyword in disaster_keywords)
                        if not has_keyword:
                            print(f"      B·ªè qua b√†i kh√¥ng li√™n quan: {title[:50]}...")
                            return None

                # Tr√≠ch xu·∫•t ng√†y
                if date == "N/A":
                    # Th·ª≠ c√°c meta tag v√† time tag
                    date_selectors = [
                        ('meta', {'property': 'article:published_time'}),
                        ('meta', {'name': 'pubdate'}),
                        ('meta', {'property': 'datePublished'}),
                        ('meta', {'name': 'publish_date'}),
                        ('meta', {'itemprop': 'datePublished'}),
                        ('time', {}),
                        ('span', {'class': re.compile(r'date|time|datetime', re.I)}),
                        ('div', {'class': re.compile(r'date|time|datetime', re.I)}),
                    ]

                    for tag, attrs in date_selectors:
                        element = soup.find(tag, attrs)
                        if element:
                            if tag == 'meta' and 'content' in element.attrs:
                                date_str = element['content']
                            else:
                                date_str = element.get('datetime', element.get_text(strip=True))

                            # T√¨m ng√†y trong chu·ªói
                            date_match = re.search(r'\d{4}-\d{2}-\d{2}|\d{2}/\d{2}/\d{4}|\d{2}-\d{2}-\d{4}', date_str)
                            if date_match:
                                date_found = date_match.group(0)
                                # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng
                                if '/' in date_found:
                                    day, month, year = date_found.split('/')
                                    date = f"{year}-{month}-{day}"
                                elif '-' in date_found and len(date_found.split('-')[0]) == 2:
                                    day, month, year = date_found.split('-')
                                    date = f"{year}-{month}-{day}"
                                else:
                                    date = date_found
                                break

                return {
                    'url': url,
                    'title': title[:500],  # Gi·ªõi h·∫°n ƒë·ªô d√†i
                    'content': content[:10000],  # Gi·ªõi h·∫°n n·ªôi dung
                    'date': date,
                    'source': source_name,
                    'content_length': len(content),
                    'scrape_time': datetime.now().isoformat()
                }

            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"      Th·ª≠ l·∫°i ({attempt + 1}/{max_retries})...")
                    time.sleep(2)
                else:
                    print(f"      L·ªói tr√≠ch xu·∫•t {url[:80]}: {e}")
                    return None

    def scrape_all_sources(self, disaster_type, category, max_articles_per_source=20, date_from=None, date_to=None):
        """Thu th·∫≠p t·ª´ t·∫•t c·∫£ c√°c ngu·ªìn cho m·ªôt lo·∫°i thi√™n tai"""
        print(f"\n{'#'*80}")
        print(f"Thi√™n tai: {disaster_type}")
        print(f"Nh√≥m: {category}")
        if date_from or date_to:
            print(f"Kho·∫£ng th·ªùi gian: {date_from or 'Kh√¥ng gi·ªõi h·∫°n'} ƒë·∫øn {date_to or 'Kh√¥ng gi·ªõi h·∫°n'}")
        print(f"{'#'*80}")

        source_results = []

        def process_source(source_key, source_config):
            try:
                print(f"\n{'='*80}")
                print(f"Ngu·ªìn: {source_config['name']} | Truy v·∫•n: {disaster_type}")
                print(f"{'='*80}")

                # Thu th·∫≠p URLs
                urls = self.extract_urls_from_source(
                    source_key,
                    disaster_type,
                    max_pages=source_config['max_pages']
                )

                # Gi·ªõi h·∫°n s·ªë b√†i vi·∫øt
                urls = urls[:max_articles_per_source]

                if not urls:
                    print(f"  Kh√¥ng t√¨m th·∫•y URL n√†o t·ª´ {source_config['name']}")
                    return []

                print(f"  Tr√≠ch xu·∫•t n·ªôi dung t·ª´ {source_config['name']}...")
                successful_articles = 0
                articles = []

                for idx, url in enumerate(urls, 1):
                    print(f"    [{idx}/{len(urls)}] {url[:70]}...")

                    article_data = self.extract_content_and_date(url, source_config['name'], disaster_type)
                    if article_data:
                        # Ki·ªÉm tra ng√†y th√°ng
                        if date_from or date_to:
                            article_date = article_data.get('date')
                            if article_date:
                                try:
                                    article_date_obj = datetime.strptime(article_date, "%Y-%m-%d").date()
                                    if date_from and article_date_obj < date_from:
                                        continue
                                    if date_to and article_date_obj > date_to:
                                        continue
                                except ValueError:
                                    pass  # B·ªè qua n·∫øu kh√¥ng parse ƒë∆∞·ª£c date

                        article_data['category'] = category
                        article_data['disaster_type'] = disaster_type
                        articles.append(article_data)
                        successful_articles += 1

                    time.sleep(0.5)  # Gi√£n c√°ch ƒë·ªÉ tr√°nh b·ªã block

                print(f"  ‚úì {source_config['name']}: {successful_articles}/{len(urls)} b√†i th√†nh c√¥ng")
                return articles

            except Exception as e:
                print(f"  ‚úó L·ªói v·ªõi {source_config['name']}: {e}")
                return []

        # S·ª≠ d·ª•ng ƒëa lu·ªìng ƒë·ªÉ x·ª≠ l√Ω c√°c ngu·ªìn song song
        with ThreadPoolExecutor(max_workers=3) as executor:  # Gi·ªõi h·∫°n 3 lu·ªìng ƒë·ªÉ tr√°nh qu√° t·∫£i
            futures = {executor.submit(process_source, source_key, source_config): source_key 
                      for source_key, source_config in NEWS_SOURCES.items()}
            
            for future in as_completed(futures):
                source_results.extend(future.result())
                time.sleep(1)  # Gi√£n c√°ch gi·ªØa c√°c ngu·ªìn

        self.results.extend(source_results)
        return source_results

    def scrape_all_disasters(self, max_articles_per_source=10):
        """Thu th·∫≠p t·∫•t c·∫£ c√°c lo·∫°i thi√™n tai"""
        print("\n" + "="*80)
        print("B·∫ÆT ƒê·∫¶U THU TH·∫¨P D·ªÆ LI·ªÜU T·ª™ 10 NGU·ªíN B√ÅO")
        print("="*80)

        total_start = time.time()

        for category, disaster_type in DISASTER_TYPES:
            print(f"\n\n{'#'*80}")
            print(f"ƒêang thu th·∫≠p: {disaster_type}")
            print(f"Thu·ªôc nh√≥m: {category}")
            print(f"{'#'*80}")

            start_time = time.time()
            self.scrape_all_sources(disaster_type, category, max_articles_per_source)
            elapsed = time.time() - start_time

            print(f"\n‚è±Ô∏è  Ho√†n th√†nh '{disaster_type}' trong {elapsed:.1f} gi√¢y")
            print(f"üìä T·ªïng s·ªë b√†i hi·ªán t·∫°i: {len(self.results)}")

            time.sleep(3)  # Gi√£n c√°ch gi·ªØa c√°c lo·∫°i thi√™n tai

        total_elapsed = time.time() - total_start
        print(f"\n{'='*80}")
        print(f"üéØ HO√ÄN T·∫§T THU TH·∫¨P T·∫§T C·∫¢ THI√äN TAI!")
        print(f"‚è±Ô∏è  T·ªïng th·ªùi gian: {total_elapsed:.1f} gi√¢y")
        print(f"üìä T·ªïng s·ªë b√†i vi·∫øt: {len(self.results)}")
        print(f"{'='*80}")

        self.save_results()
        self.print_statistics()

        return self.results

    def debug_extraction(self, source_key, query, page=1):
        """Debug chi ti·∫øt qu√° tr√¨nh tr√≠ch xu·∫•t"""
        source = NEWS_SOURCES[source_key]
        formatted_query = quote(query)

        if '{page}' in source['search_url']:
            search_url = source['search_url'].format(query=formatted_query, page=page)
        else:
            search_url = source['search_url'].format(query=formatted_query)

        print(f"\n{'='*80}")
        print(f"DEBUG EXTRACTION: {source['name']}")
        print(f"URL: {search_url}")
        print(f"Query: {query}")
        print(f"Page: {page}")
        print(f"{'='*80}")

        try:
            verify_ssl = not source.get('needs_ssl_workaround', False)
            response = self.session.get(search_url, timeout=15, verify=verify_ssl)

            print(f"Status Code: {response.status_code}")
            print(f"Encoding: {response.encoding}")
            print(f"Content Length: {len(response.text)} chars")

            soup = BeautifulSoup(response.text, 'html.parser')

            # 1. Test selector ch√≠nh
            print(f"\n1. Test selector ch√≠nh: {source['article_selector']}")
            articles = soup.select(source['article_selector'])
            print(f"   T√¨m th·∫•y: {len(articles)} elements")

            if articles:
                for i, a in enumerate(articles[:5], 1):
                    href = a.get('href', 'No href')
                    text = a.get_text(strip=True)[:100]
                    print(f"   [{i}] Text: {text}")
                    print(f"       Href: {href[:100]}")

            # 2. T√¨m t·∫•t c·∫£ th·∫ª H2, H3, H4
            print(f"\n2. T√¨m t·∫•t c·∫£ th·∫ª ti√™u ƒë·ªÅ:")
            for tag in ['h1', 'h2', 'h3', 'h4']:
                tags = soup.find_all(tag)
                print(f"   {tag.upper()}: {len(tags)} th·∫ª")
                for i, h in enumerate(tags[:2], 1):
                    link = h.find('a')
                    if link:
                        print(f"     [{i}] Text: {h.get_text(strip=True)[:80]}")
                        print(f"         Link: {link.get('href', '')[:80]}")

            # 3. T√¨m t·∫•t c·∫£ links
            print(f"\n3. T·ªïng s·ªë links tr√™n trang:")
            all_links = soup.find_all('a', href=True)
            print(f"   T·∫•t c·∫£ links: {len(all_links)}")

            # 4. HTML snippet c·ªßa container c√≥ th·ªÉ ch·ª©a k·∫øt qu·∫£
            print(f"\n4. T√¨m container k·∫øt qu·∫£ t√¨m ki·∫øm:")
            container_selectors = [
                '.search-results', '.results', '.list-news',
                '.news-list', '.article-list', '.story-list',
                '[class*="search"]', '[class*="result"]',
                'main', 'section', '.content'
            ]

            for selector in container_selectors:
                containers = soup.select(selector)
                if containers:
                    print(f"   Selector '{selector}': {len(containers)} containers")
                    for i, container in enumerate(containers[:1], 1):
                        print(f"   Container {i} HTML (500 chars):")
                        print(f"   {str(container)[:500]}...")
                        break

            # 5. In m·ªôt ph·∫ßn HTML ƒë·ªÉ ki·ªÉm tra
            print(f"\n5. M·∫´u HTML trang (1000 k√Ω t·ª± ƒë·∫ßu):")
            print(response.text[:1000])

        except Exception as e:
            print(f"L·ªói: {e}")
            import traceback
            traceback.print_exc()

    def debug_source(self, source_key, query):
        """Debug m·ªôt ngu·ªìn c·ª• th·ªÉ"""
        self.debug_extraction(source_key, query, 1)

    def save_results(self):
        """L∆∞u k·∫øt qu·∫£ ra file"""
        if not self.results:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u!")
            return None, None

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # L∆∞u JSON
        json_filename = os.path.join(DATA_DIR, f"disaster_data_multisource_{timestamp}.json")
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\n‚úì ƒê√£ l∆∞u JSON: {json_filename}")

        # L∆∞u CSV
        try:
            df = pd.DataFrame(self.results)
            csv_filename = os.path.join(DATA_DIR, f"disaster_data_multisource_{timestamp}.csv")
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"‚úì ƒê√£ l∆∞u CSV: {csv_filename}")
        except Exception as e:
            print(f"‚úó L·ªói l∆∞u CSV: {e}")
            csv_filename = None

        return json_filename, csv_filename

    def print_statistics(self):
        """In th·ªëng k√™ chi ti·∫øt"""
        if not self.results:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ th·ªëng k√™!")
            return

        print("\n" + "="*80)
        print("üìä TH·ªêNG K√ä THU TH·∫¨P CHI TI·∫æT")
        print("="*80)

        df = pd.DataFrame(self.results)

        print(f"\nüìà T·ªïng s·ªë b√†i vi·∫øt: {len(df)}")

        print("\n--- Theo ngu·ªìn b√°o ---")
        source_stats = df.groupby('source').size().sort_values(ascending=False)
        for source, count in source_stats.items():
            percentage = (count / len(df)) * 100
            print(f"  üì∞ {source}: {count} b√†i ({percentage:.1f}%)")

        print("\n--- Theo lo·∫°i thi√™n tai ---")
        disaster_stats = df.groupby('disaster_type').size().sort_values(ascending=False)
        for disaster, count in disaster_stats.items():
            print(f"  ‚ö†Ô∏è  {disaster}: {count} b√†i")

        print("\n--- Theo nh√≥m thi√™n tai ---")
        category_stats = df.groupby('category').size().sort_values(ascending=False)
        for cat, count in category_stats.items():
            print(f"  üìÅ {cat}: {count} b√†i")

        print("\n--- Theo ng√†y th√°ng ---")
        if 'date' in df.columns:
            date_counts = df['date'].value_counts().head(10)
            for date, count in date_counts.items():
                print(f"  üìÖ {date}: {count} b√†i")

        print("\n--- ƒê·ªô d√†i n·ªôi dung ---")
        if 'content_length' in df.columns:
            avg_length = df['content_length'].mean()
            max_length = df['content_length'].max()
            min_length = df['content_length'].min()
            print(f"  üìù Trung b√¨nh: {avg_length:.0f} k√Ω t·ª±")
            print(f"  üìù Ng·∫Øn nh·∫•t: {min_length} k√Ω t·ª±")
            print(f"  üìù D√†i nh·∫•t: {max_length} k√Ω t·ª±")

        # M·∫´u d·ªØ li·ªáu
        print("\n" + "="*80)
        print("üìã M·∫™U D·ªÆ LI·ªÜU (3 b√†i ƒë·∫ßu ti√™n)")
        print("="*80)

        for i, item in enumerate(self.results[:3], 1):
            print(f"\n[{i}] Ngu·ªìn: {item['source']}")
            print(f"    Lo·∫°i: {item['disaster_type']}")
            print(f"    Ti√™u ƒë·ªÅ: {item['title'][:100]}...")
            print(f"    Ng√†y: {item['date']}")
            print(f"    URL: {item['url'][:80]}...")
            print(f"    ƒê·ªô d√†i n·ªôi dung: {item.get('content_length', 'N/A')} k√Ω t·ª±")

    def quick_test(self):
        """Ki·ªÉm tra nhanh t·∫•t c·∫£ c√°c ngu·ªìn"""
        print("\n" + "="*80)
        print("üîç KI·ªÇM TRA NHANH T·∫§T C·∫¢ NGU·ªíN B√ÅO")
        print("="*80)

        test_query = "B√£o"
        test_results = {}

        for source_key in NEWS_SOURCES.keys():
            print(f"\n--- Test {NEWS_SOURCES[source_key]['name']} ---")
            try:
                urls = self.extract_urls_from_source(source_key, test_query, max_pages=1)
                if urls:
                    test_results[source_key] = {
                        'status': 'OK',
                        'urls_found': len(urls),
                        'sample_url': list(urls)[0] if urls else None
                    }
                    print(f"‚úÖ OK: T√¨m th·∫•y {len(urls)} URLs")
                    print(f"   V√≠ d·ª•: {list(urls)[0][:80]}...")
                else:
                    test_results[source_key] = {
                        'status': 'NO_URLS',
                        'urls_found': 0,
                        'sample_url': None
                    }
                    print(f"‚ö†Ô∏è  C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y URL n√†o")
            except Exception as e:
                test_results[source_key] = {
                    'status': 'ERROR',
                    'error': str(e)[:100],
                    'urls_found': 0
                }
                print(f"‚ùå L·ªñI: {str(e)[:100]}")

            time.sleep(1)

        # T·ªïng k·∫øt
        print("\n" + "="*80)
        print("üìã T·ªîNG K·∫æT KI·ªÇM TRA")
        print("="*80)

        ok_count = sum(1 for r in test_results.values() if r['status'] == 'OK')
        warning_count = sum(1 for r in test_results.values() if r['status'] == 'NO_URLS')
        error_count = sum(1 for r in test_results.values() if r['status'] == 'ERROR')

        print(f"\n‚úÖ Ho·∫°t ƒë·ªông t·ªët: {ok_count}/{len(NEWS_SOURCES)} ngu·ªìn")
        print(f"‚ö†Ô∏è  C·∫£nh b√°o: {warning_count}/{len(NEWS_SOURCES)} ngu·ªìn")
        print(f"‚ùå L·ªói: {error_count}/{len(NEWS_SOURCES)} ngu·ªìn")

        if error_count > 0:
            print("\nüîß C√°c ngu·ªìn c·∫ßn debug:")
            for source_key, result in test_results.items():
                if result['status'] == 'ERROR':
                    print(f"   - {NEWS_SOURCES[source_key]['name']}: {result.get('error', 'L·ªói kh√¥ng x√°c ƒë·ªãnh')}")

# Ch·∫°y ch∆∞∆°ng tr√¨nh
if __name__ == "__main__":
    scraper = NewsScraperMultiSource()

    # === MENU CH√çNH ===
    print("\n" + "="*80)
    print("üå™Ô∏è  H·ªÜ TH·ªêNG THU TH·∫¨P D·ªÆ LI·ªÜU THI√äN TAI ƒêA NGU·ªíN")
    print("="*80)

    while True:
        print("\n" + "-"*80)
        print("MENU CH√çNH:")
        print("1. Ki·ªÉm tra nhanh t·∫•t c·∫£ ngu·ªìn")
        print("2. Debug m·ªôt ngu·ªìn c·ª• th·ªÉ")
        print("3. Thu th·∫≠p d·ªØ li·ªáu m·∫´u (1 lo·∫°i thi√™n tai)")
        print("4. Thu th·∫≠p to√†n b·ªô d·ªØ li·ªáu thi√™n tai")
        print("5. Xem th·ªëng k√™ (n·∫øu c√≥ d·ªØ li·ªáu)")
        print("6. Tho√°t")
        print("-"*80)

        choice = input("Ch·ªçn ch·ª©c nƒÉng (1-6): ").strip()

        if choice == "1":
            # Ki·ªÉm tra nhanh
            scraper.quick_test()

        elif choice == "2":
            # Debug m·ªôt ngu·ªìn
            print("\nC√°c ngu·ªìn c√≥ s·∫µn:")
            for i, (key, config) in enumerate(NEWS_SOURCES.items(), 1):
                print(f"{i}. {config['name']} ({key})")

            try:
                source_num = int(input("Ch·ªçn s·ªë ngu·ªìn (1-10): "))
                source_keys = list(NEWS_SOURCES.keys())
                if 1 <= source_num <= len(source_keys):
                    source_key = source_keys[source_num - 1]
                    query = input("Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm (m·∫∑c ƒë·ªãnh: B√£o): ") or "B√£o"
                    scraper.debug_source(source_key, query)
                else:
                    print("S·ªë kh√¥ng h·ª£p l·ªá!")
            except ValueError:
                print("Vui l√≤ng nh·∫≠p s·ªë!")

        elif choice == "3":
            # Thu th·∫≠p d·ªØ li·ªáu m·∫´u
            print("\nCh·ªçn lo·∫°i thi√™n tai m·∫´u:")
            print("1. B√£o, √°p th·∫•p nhi·ªát ƒë·ªõi")
            print("2. L≈©, l≈© qu√©t")
            print("3. H·∫°n h√°n")
            print("4. S·∫°t l·ªü ƒë·∫•t")
            print("5. Ch√°y r·ª´ng")

            disaster_choice = input("Ch·ªçn s·ªë (1-5, m·∫∑c ƒë·ªãnh: 1): ").strip() or "1"

            disasters_map = {
                "1": "B√£o, √°p th·∫•p nhi·ªát ƒë·ªõi",
                "2": "L≈©, l≈© qu√©t",
                "3": "H·∫°n h√°n",
                "4": "S·∫°t l·ªü ƒë·∫•t, tr∆∞·ª£t ƒë·∫•t, s·ª•t l√∫n",
                "5": "Ch√°y r·ª´ng"
            }

            if disaster_choice in disasters_map:
                disaster_type = disasters_map[disaster_choice]
                category = "Thi√™n tai kh√≠ t∆∞·ª£ng ‚Äì th·ªßy vƒÉn" if disaster_choice in ["1", "2", "3"] else \
                          "Thi√™n tai ƒë·ªãa ch·∫•t" if disaster_choice == "4" else \
                          "Thi√™n tai m√¥i tr∆∞·ªùng ‚Äì con ng∆∞·ªùi g√¢y ra"

                try:
                    max_articles = int(input("S·ªë b√†i t·ªëi ƒëa m·ªói ngu·ªìn (m·∫∑c ƒë·ªãnh: 10): ") or "10")
                except ValueError:
                    max_articles = 10

                date_from = input("Ng√†y b·∫Øt ƒë·∫ßu (YYYY-MM-DD, m·∫∑c ƒë·ªãnh: kh√¥ng gi·ªõi h·∫°n): ").strip() or None
                date_to = input("Ng√†y k·∫øt th√∫c (YYYY-MM-DD, m·∫∑c ƒë·ªãnh: kh√¥ng gi·ªõi h·∫°n): ").strip() or None

                if date_from:
                    try:
                        date_from = datetime.strptime(date_from, "%Y-%m-%d").date()
                    except ValueError:
                        print("ƒê·ªãnh d·∫°ng ng√†y kh√¥ng h·ª£p l·ªá, b·ªè qua.")
                        date_from = None
                if date_to:
                    try:
                        date_to = datetime.strptime(date_to, "%Y-%m-%d").date()
                    except ValueError:
                        print("ƒê·ªãnh d·∫°ng ng√†y kh√¥ng h·ª£p l·ªá, b·ªè qua.")
                        date_to = None

                print(f"\nƒêang thu th·∫≠p d·ªØ li·ªáu cho: {disaster_type}...")
                results = scraper.scrape_all_sources(
                    disaster_type=disaster_type,
                    category=category,
                    max_articles_per_source=max_articles,
                    date_from=date_from,
                    date_to=date_to
                )
                scraper.save_results()
                scraper.print_statistics()
            else:
                print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")

        elif choice == "4":
            # Thu th·∫≠p to√†n b·ªô
            confirm = input("C·∫¢NH B√ÅO: Thao t√°c n√†y c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian. Ti·∫øp t·ª•c? (y/n): ")
            if confirm.lower() == 'y':
                try:
                    max_articles = int(input("S·ªë b√†i t·ªëi ƒëa m·ªói ngu·ªìn cho m·ªói thi√™n tai (m·∫∑c ƒë·ªãnh: 5): ") or "5")
                except ValueError:
                    max_articles = 5

                scraper.scrape_all_disasters(max_articles_per_source=max_articles)

        elif choice == "5":
            # Xem th·ªëng k√™
            if scraper.results:
                scraper.print_statistics()
            else:
                print("Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng thu th·∫≠p tr∆∞·ªõc.")

        elif choice == "6":
            print("Tho√°t ch∆∞∆°ng tr√¨nh.")
            break

        else:
            print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")

        # H·ªèi ti·∫øp t·ª•c hay kh√¥ng
        if choice in ["3", "4", "5"]:
            continue_option = input("\nTi·∫øp t·ª•c v·ªõi menu ch√≠nh? (y/n): ")
            if continue_option.lower() != 'y':
                print("Tho√°t ch∆∞∆°ng tr√¨nh.")
                break